List of All CHI 2015 Interactivity Final Submissions

"ID","Decision","Title","Contact given name","Contact family name","Contact Email","Document","Page length","Page size","Non-embedded fonts","Incomplete","Author list","Author emails","Author ID 1"," Author given first name 1","Middle initial or name 1","Author last/family name 1","Valid email address 1","Primary Affiliation (no labs or dept names in this field) 1 - Department/School/Lab","Primary Affiliation (no labs or dept names in this field) 1 - Institution","Primary Affiliation (no labs or dept names in this field) 1 - City","Primary Affiliation (no labs or dept names in this field) 1 - State or Province","Primary Affiliation (no labs or dept names in this field) 1 - Country","Secondary Affiliation (optional) (no labs or dept names in this field) 1 - Department/School/Lab","Secondary Affiliation (optional) (no labs or dept names in this field) 1 - Institution","Secondary Affiliation (optional) (no labs or dept names in this field) 1 - City","Secondary Affiliation (optional) (no labs or dept names in this field) 1 - State or Province","Secondary Affiliation (optional) (no labs or dept names in this field) 1 - Country","Author ID 2"," Author given first name 2","Middle initial or name 2","Author last/family name 2","Valid email address 2","Primary Affiliation (no labs or dept names in this field) 2 - Department/School/Lab","Primary Affiliation (no labs or dept names in this field) 2 - Institution","Primary Affiliation (no labs or dept names in this field) 2 - City","Primary Affiliation (no labs or dept names in this field) 2 - State or Province","Primary Affiliation (no labs or dept names in this field) 2 - Country","Secondary Affiliation (optional) (no labs or dept names in this field) 2 - Department/School/Lab","Secondary Affiliation (optional) (no labs or dept names in this field) 2 - Institution","Secondary Affiliation (optional) (no labs or dept names in this field) 2 - City","Secondary Affiliation (optional) (no labs or dept names in this field) 2 - State or Province","Secondary Affiliation (optional) (no labs or dept names in this field) 2 - Country","Author ID 3"," Author given first name 3","Middle initial or name 3","Author last/family name 3","Valid email address 3","Primary Affiliation (no labs or dept names in this field) 3 - Department/School/Lab","Primary Affiliation (no labs or dept names in this field) 3 - Institution","Primary Affiliation (no labs or dept names in this field) 3 - City","Primary Affiliation (no labs or dept names in this field) 3 - State or Province","Primary Affiliation (no labs or dept names in this field) 3 - Country","Secondary Affiliation (optional) (no labs or dept names in this field) 3 - Department/School/Lab","Secondary Affiliation (optional) (no labs or dept names in this field) 3 - Institution","Secondary Affiliation (optional) (no labs or dept names in this field) 3 - City","Secondary Affiliation (optional) (no labs or dept names in this field) 3 - State or Province","Secondary Affiliation (optional) (no labs or dept names in this field) 3 - Country","Author ID 4"," Author given first name 4","Middle initial or name 4","Author last/family name 4","Valid email address 4","Primary Affiliation (no labs or dept names in this field) 4 - Department/School/Lab","Primary Affiliation (no labs or dept names in this field) 4 - Institution","Primary Affiliation (no labs or dept names in this field) 4 - City","Primary Affiliation (no labs or dept names in this field) 4 - State or Province","Primary Affiliation (no labs or dept names in this field) 4 - Country","Secondary Affiliation (optional) (no labs or dept names in this field) 4 - Department/School/Lab","Secondary Affiliation (optional) (no labs or dept names in this field) 4 - Institution","Secondary Affiliation (optional) (no labs or dept names in this field) 4 - City","Secondary Affiliation (optional) (no labs or dept names in this field) 4 - State or Province","Secondary Affiliation (optional) (no labs or dept names in this field) 4 - Country","Author ID 5"," Author given first name 5","Middle initial or name 5","Author last/family name 5","Valid email address 5","Primary Affiliation (no labs or dept names in this field) 5 - Department/School/Lab","Primary Affiliation (no labs or dept names in this field) 5 - Institution","Primary Affiliation (no labs or dept names in this field) 5 - City","Primary Affiliation (no labs or dept names in this field) 5 - State or Province","Primary Affiliation (no labs or dept names in this field) 5 - Country","Secondary Affiliation (optional) (no labs or dept names in this field) 5 - Department/School/Lab","Secondary Affiliation (optional) (no labs or dept names in this field) 5 - Institution","Secondary Affiliation (optional) (no labs or dept names in this field) 5 - City","Secondary Affiliation (optional) (no labs or dept names in this field) 5 - State or Province","Secondary Affiliation (optional) (no labs or dept names in this field) 5 - Country","Author ID 6"," Author given first name 6","Middle initial or name 6","Author last/family name 6","Valid email address 6","Primary Affiliation (no labs or dept names in this field) 6 - Department/School/Lab","Primary Affiliation (no labs or dept names in this field) 6 - Institution","Primary Affiliation (no labs or dept names in this field) 6 - City","Primary Affiliation (no labs or dept names in this field) 6 - State or Province","Primary Affiliation (no labs or dept names in this field) 6 - Country","Secondary Affiliation (optional) (no labs or dept names in this field) 6 - Department/School/Lab","Secondary Affiliation (optional) (no labs or dept names in this field) 6 - Institution","Secondary Affiliation (optional) (no labs or dept names in this field) 6 - City","Secondary Affiliation (optional) (no labs or dept names in this field) 6 - State or Province","Secondary Affiliation (optional) (no labs or dept names in this field) 6 - Country","Author ID 7"," Author given first name 7","Middle initial or name 7","Author last/family name 7","Valid email address 7","Primary Affiliation (no labs or dept names in this field) 7 - Department/School/Lab","Primary Affiliation (no labs or dept names in this field) 7 - Institution","Primary Affiliation (no labs or dept names in this field) 7 - City","Primary Affiliation (no labs or dept names in this field) 7 - State or Province","Primary Affiliation (no labs or dept names in this field) 7 - Country","Secondary Affiliation (optional) (no labs or dept names in this field) 7 - Department/School/Lab","Secondary Affiliation (optional) (no labs or dept names in this field) 7 - Institution","Secondary Affiliation (optional) (no labs or dept names in this field) 7 - City","Secondary Affiliation (optional) (no labs or dept names in this field) 7 - State or Province","Secondary Affiliation (optional) (no labs or dept names in this field) 7 - Country","Author ID 8"," Author given first name 8","Middle initial or name 8","Author last/family name 8","Valid email address 8","Primary Affiliation (no labs or dept names in this field) 8 - Department/School/Lab","Primary Affiliation (no labs or dept names in this field) 8 - Institution","Primary Affiliation (no labs or dept names in this field) 8 - City","Primary Affiliation (no labs or dept names in this field) 8 - State or Province","Primary Affiliation (no labs or dept names in this field) 8 - Country","Secondary Affiliation (optional) (no labs or dept names in this field) 8 - Department/School/Lab","Secondary Affiliation (optional) (no labs or dept names in this field) 8 - Institution","Secondary Affiliation (optional) (no labs or dept names in this field) 8 - City","Secondary Affiliation (optional) (no labs or dept names in this field) 8 - State or Province","Secondary Affiliation (optional) (no labs or dept names in this field) 8 - Country","Author ID 9"," Author given first name 9","Middle initial or name 9","Author last/family name 9","Valid email address 9","Primary Affiliation (no labs or dept names in this field) 9 - Department/School/Lab","Primary Affiliation (no labs or dept names in this field) 9 - Institution","Primary Affiliation (no labs or dept names in this field) 9 - City","Primary Affiliation (no labs or dept names in this field) 9 - State or Province","Primary Affiliation (no labs or dept names in this field) 9 - Country","Secondary Affiliation (optional) (no labs or dept names in this field) 9 - Department/School/Lab","Secondary Affiliation (optional) (no labs or dept names in this field) 9 - Institution","Secondary Affiliation (optional) (no labs or dept names in this field) 9 - City","Secondary Affiliation (optional) (no labs or dept names in this field) 9 - State or Province","Secondary Affiliation (optional) (no labs or dept names in this field) 9 - Country","Author ID 10"," Author given first name 10","Middle initial or name 10","Author last/family name 10","Valid email address 10","Primary Affiliation (no labs or dept names in this field) 10 - Department/School/Lab","Primary Affiliation (no labs or dept names in this field) 10 - Institution","Primary Affiliation (no labs or dept names in this field) 10 - City","Primary Affiliation (no labs or dept names in this field) 10 - State or Province","Primary Affiliation (no labs or dept names in this field) 10 - Country","Secondary Affiliation (optional) (no labs or dept names in this field) 10 - Department/School/Lab","Secondary Affiliation (optional) (no labs or dept names in this field) 10 - Institution","Secondary Affiliation (optional) (no labs or dept names in this field) 10 - City","Secondary Affiliation (optional) (no labs or dept names in this field) 10 - State or Province","Secondary Affiliation (optional) (no labs or dept names in this field) 10 - Country","Author ID 11"," Author given first name 11","Middle initial or name 11","Author last/family name 11","Valid email address 11","Primary Affiliation (no labs or dept names in this field) 11 - Department/School/Lab","Primary Affiliation (no labs or dept names in this field) 11 - Institution","Primary Affiliation (no labs or dept names in this field) 11 - City","Primary Affiliation (no labs or dept names in this field) 11 - State or Province","Primary Affiliation (no labs or dept names in this field) 11 - Country","Secondary Affiliation (optional) (no labs or dept names in this field) 11 - Department/School/Lab","Secondary Affiliation (optional) (no labs or dept names in this field) 11 - Institution","Secondary Affiliation (optional) (no labs or dept names in this field) 11 - City","Secondary Affiliation (optional) (no labs or dept names in this field) 11 - State or Province","Secondary Affiliation (optional) (no labs or dept names in this field) 11 - Country","Author ID 12"," Author given first name 12","Middle initial or name 12","Author last/family name 12","Valid email address 12","Primary Affiliation (no labs or dept names in this field) 12 - Department/School/Lab","Primary Affiliation (no labs or dept names in this field) 12 - Institution","Primary Affiliation (no labs or dept names in this field) 12 - City","Primary Affiliation (no labs or dept names in this field) 12 - State or Province","Primary Affiliation (no labs or dept names in this field) 12 - Country","Secondary Affiliation (optional) (no labs or dept names in this field) 12 - Department/School/Lab","Secondary Affiliation (optional) (no labs or dept names in this field) 12 - Institution","Secondary Affiliation (optional) (no labs or dept names in this field) 12 - City","Secondary Affiliation (optional) (no labs or dept names in this field) 12 - State or Province","Secondary Affiliation (optional) (no labs or dept names in this field) 12 - Country","Author ID 13"," Author given first name 13","Middle initial or name 13","Author last/family name 13","Valid email address 13","Primary Affiliation (no labs or dept names in this field) 13 - Department/School/Lab","Primary Affiliation (no labs or dept names in this field) 13 - Institution","Primary Affiliation (no labs or dept names in this field) 13 - City","Primary Affiliation (no labs or dept names in this field) 13 - State or Province","Primary Affiliation (no labs or dept names in this field) 13 - Country","Secondary Affiliation (optional) (no labs or dept names in this field) 13 - Department/School/Lab","Secondary Affiliation (optional) (no labs or dept names in this field) 13 - Institution","Secondary Affiliation (optional) (no labs or dept names in this field) 13 - City","Secondary Affiliation (optional) (no labs or dept names in this field) 13 - State or Province","Secondary Affiliation (optional) (no labs or dept names in this field) 13 - Country","Author ID 14"," Author given first name 14","Middle initial or name 14","Author last/family name 14","Valid email address 14","Primary Affiliation (no labs or dept names in this field) 14 - Department/School/Lab","Primary Affiliation (no labs or dept names in this field) 14 - Institution","Primary Affiliation (no labs or dept names in this field) 14 - City","Primary Affiliation (no labs or dept names in this field) 14 - State or Province","Primary Affiliation (no labs or dept names in this field) 14 - Country","Secondary Affiliation (optional) (no labs or dept names in this field) 14 - Department/School/Lab","Secondary Affiliation (optional) (no labs or dept names in this field) 14 - Institution","Secondary Affiliation (optional) (no labs or dept names in this field) 14 - City","Secondary Affiliation (optional) (no labs or dept names in this field) 14 - State or Province","Secondary Affiliation (optional) (no labs or dept names in this field) 14 - Country","Author ID 15"," Author given first name 15","Middle initial or name 15","Author last/family name 15","Valid email address 15","Primary Affiliation (no labs or dept names in this field) 15 - Department/School/Lab","Primary Affiliation (no labs or dept names in this field) 15 - Institution","Primary Affiliation (no labs or dept names in this field) 15 - City","Primary Affiliation (no labs or dept names in this field) 15 - State or Province","Primary Affiliation (no labs or dept names in this field) 15 - Country","Secondary Affiliation (optional) (no labs or dept names in this field) 15 - Department/School/Lab","Secondary Affiliation (optional) (no labs or dept names in this field) 15 - Institution","Secondary Affiliation (optional) (no labs or dept names in this field) 15 - City","Secondary Affiliation (optional) (no labs or dept names in this field) 15 - State or Province","Secondary Affiliation (optional) (no labs or dept names in this field) 15 - Country","Abstract","Contact Author Name","Contact Author Email Address","References","Author Keywords","ACM Classifications","Document Source","Thumbnail Image","Video Figure","Sign Image","Video Preview","Thumbnail Image Caption","Program Description (Mandatory Field)","Related Accepted Paper or Note ID","Program Number","Sign Image Title","Presenting Author","Backup Presenting Author","Video Preview Early Release (Required)","Content Complete (AC use only)","Format Complete (Publications chair use only)","Grant Number 1","Grant Agency 1","Grant Agency 2","Grant Number 2","Grant Agency 3","Grant Number 3","Last Update","Notes"

"int102","A","Wrist Compression Feedback by Pneumatic Actuation","Henning","Pohl","HenningPohl@gmail.com","int0102-paper.pdf","4","letter","","","Henning Pohl, Dennis Becke, Eugen Wagner, Maximilian Schrapel, Michael Rohs","Henning@hci.uni-hannover.de, dennis.becke@web.de, eugen_wagner@hotmail.de, maxud@gmx.net, michael.rohs@hci.uni-hannover.de","17146","Henning","","Pohl","Henning@hci.uni-hannover.de","","University of Hannover","Hannover","","Germany","","","","","","45902","Dennis","","Becke","dennis.becke@web.de","","University of Hannover","Hannover","","Germany","","","","","","45903","Eugen","","Wagner","eugen_wagner@hotmail.de","","University of Hannover","Hannover","","Germany","","","","","","45904","Maximilian","","Schrapel","maxud@gmx.net","","University of Hannover","Hannover","","Germany","","","","","","6770","Michael","","Rohs","michael.rohs@hci.uni-hannover.de","","University of Hannover","Hannover","","Germany","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Most common forms of haptic feedback use vibration, which immediately captures the user's attention, yet is limited in the range of strengths it can achieve. Vibration feedback over extended periods also tends to be annoying. We present compression feedback, a form of haptic feedback that scales from very subtle to very strong and is able to provide sustained stimuli and pressure patterns. The demonstration may serve as an inspiration for further work in this area, applying compression feedback to generate subtle, intimate, as well as intense feedback.","Henning Pohl","HenningPohl@gmail.com","1. Baumann, M. Emulating Human Attention-Getting Practices with Wearable Haptics. In Proceedings of the 2010 IEEE Haptics Symposium (2010), 149–156. \ 2. Chinello, F., and Aurilio, M. The HapBand: A Cutaneous Device for Remote Tactile Interaction. In Proc. EuroHaptics ’14 (2014), 284–291. \ 3. Culjat, M., King, C.-H., Franco, M., Bisley, J., Grundfest, W., and Dutson, E. Pneumatic Balloon Actuators for Tactile Feedback in Robotic Surgery. Industrial Robot: An International Journal 35, 5 (2008), 449–455. \ 4. Enriquez, M., Afonin, O., Yager, B., and Maclean, K. A Pneumatic Tactile Alerting System for the Driving Environment. In Proc. PUI ’01 (2001), 1–7. \ 5. Fan, R. E., Culjat, M. O., King, C.-H., Franco, M. L., Boryk, R., Bisley, J. W., Dutson, E., and Grundfest, W. S. A Haptic Feedback System for Lower-Limb Prostheses. IEEE Transactions on Neural Systems and Rehabilitation Engineering 16, 3 (2008), 270–277. \ 6. Follmer, S., Leithinger, D., Olwal, A., Cheng, N., and Ishii, H. Jamming User Interfaces: Programmable Particle Stiﬀness and Sensing for Malleable and Shape-Changing Devices. In Proc. UIST ’12 (2012), 519–528. \ 7. Haller, M., Richter, C., Brandl, P., Gross, S., Schossleitner, G., Schrempf, A., Nii, H., Sugimoto, M., and Inami, M. Finding the Right Way for Interrupting People Improving Their Sitting Posture. In Proc. INTERACT ’11 (2011), 1–17. \ 8. Harrison, C., and Hudson, S. E. Providing Dynamically Changeable Physical Buttons on a Visual Display. In Proc. CHI ’09 (2009), 299–308. \ 9. Kim, Y., Kim, S., Ha, T., Oakley, I., and Woo, W. Air-Jet Button Eﬀects in AR. In Proc. ICAT’06, vol. 4282 of Lecture Notes in Computer Science (2006), 384–391. \ 10. Klatzky, R. L., and Lederman, S. J. Touch. In Experimental Psychology, I. B. Weiner, Ed., vol. 4 of Handbook of Psychology. 2002, 147–176. \ 11. Martinez, R. V., Fish, C. R., Chen, X., and Whitesides, G. M. Elastomeric Origami: Programmable Paper-Elastomer Composites as Pneumatic Actuators. Advanced Functional Materials 22, 7 (2012), 1376–1384. \ 12. Pakanen, M., Colley, A., H¨akkil¨a, J., Kildal, J., and Lantz, V. Squeezy Bracelet: Designing a Wearable Communication Device for Tactile Interaction. In Proc. NordiCHI ’14 (2014), 305–314. \ 13. Yao, L., Niiyama, R., Ou, J., Follmer, S., Della Silva, C., and Ishii, H. PneUI: Pneumatically Actuated Soft Composite Material s for Shape Changing Interfaces. In Proc. UIST ’13 (2013), 13–22. \ 14. Zheng, Y., and Morrell, J. B. Haptic Actuator Design Parameters that Inﬂuence Aﬀect and Attention. In 2012 IEEE Haptics Symposium HAPTICS’12 (2012), 463–470. \ 15. Zheng, Y., Su, E., and Morrell, J. B. Design and evaluation of pactors for managing attention capture. In 2013 World Haptics Conference (WHC) (2013), 497–502. \ ","pressure feedback, wearable, compressive feedback, blood pressure, pneumatics","H.5.2","int0102-file1.zip","","","int0102-file4.jpg","","","Strap in and feel your arm being squeezed. We use pneumatic actuation to generate several different patterns of compression feedback. Experience how this scales from subtle to intense.","","121","Wrist Compression Feedback by Pneumatic Actuation","Henning Pohl","Michael Rohs","","","FormatComplete","","","","","","","Jan 30 11:49",""
"int105","A","Harmonious Haptics: Enhanced Tactile Feedback Using a Mobile and a Wearable Device","Sungjae","Hwang","best@kaist.ac.kr","int0105-paper.pdf","4","letter","","","Sungjae Hwang, John Song, Junghyeon Gim","Sungjae.hwang@futureplay.co, John.song@futureplay.co, Junghyeon.gim@futureplay.co","47729","Sungjae","","Hwang","Sungjae.hwang@futureplay.co","Creative Lab","FuturePlay Inc.","Seoul","","Korea, Republic of","","","","","","47730","John","","Song","John.song@futureplay.co","Creative Lab","FuturePlay Inc.","Seoul","","Korea, Republic of","","","","","","47731","Junghyeon","","Gim","Junghyeon.gim@futureplay.co","Creative Lab","FuturePlay Inc.","Seoul","","Korea, Republic of","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Smartwatches now allow information to be conveniently accessed directly from the user’s wrist. However, the smartwatches currently available in the market offer a limited number of applications. In this paper, we propose a new interaction technique named Harmonious Haptics, which provides users with enhanced tactile sensations by utilizing smartwatches as additional tactile displays for smartphones. When combined with typical mobile devices, our technique enables the design of a wide variety of tactile stimuli. To illustrate the potential of our approach, we developed a set of example applications that provide users with rich tactile feedback such as feeling textures in a graphical user interface, transferring a file between the tablet and the smartwatch device, and controlling UI components.","Sungjae Hwang","sungjae.hwang@futureplay.co","1. Alles, D. S. Information Transmission by Phantom Sensation. In Proc. of Man-Machine Systems (1970), 85-91 \ 2. Chen, X. A., Grossman, T., Wigdor, D. J., and Firzmaurice, G. Duet: exploring joint interactions on a smart phone and a smart watch. In Proc. of CHI (2014), 159-168. \ 3. Funk, M., Sahami, A., Henze, N., and Schmidt, A. Using a touch-sensitive wristband for text entry on smart watches. In Proc. of CHI EA (2014), 2305-2310. \ 4. Geldard, F. A., and Sherrick, C. E. The Cutaneous Rabbit: A Perceptual Illusion. In Proc. of Science (1972), 178-179. \ 5. Knibbe, J., Plasencia, D. M., Bainbridge, C., Chan, C. K., Wu, J., Cable, T., Munir, H., and Coyle, D. Extending interaction for smart watches: enabling bimanual around device control. In Proc. of CHI EA (2014), 1891-1896. \ 6. Lee, S. C., and Starner, T. Buzzwear: Alert Perception in Wearable Tactile Displays on the Wrist. In Proc. of CHI (2010), 433-442. \ 7. Narayanaswami, C., and Raghunath, M. T. Application Design for a Smart Watch with a High Resolution Display. In Proc. of ISWC (2010), 7-14. \ 8. Paneels, S., Anastassova, M., Strachan, S., Van, S. P., Sivacoumarane, S., Bolzmacher, C. What’s Around Me? Multi-Actuator Haptic Feedback on the Wrist. In Proc. of World Haptic Conference (2013), 407-412. \ 9. Pasquero, J., Stobbe, S. J., Stonehouse, N. A Haptic Wristwatch for Eyes-Free Interactions, In Proc. of CHI (2011), 3257-3266. \ 10. Xiao, R., Laput, G., and Harrison, C. Expanding the input expressivity of smartwatches with mechanical pan, twist, tilt and click. In Proc. of CHI (2014), 193-196. \ ","Haptics; tactile illusion; smartwatch; mobile device; wearable devices","H.5.2","int0105-file1.doc","int0105-file2.jpg","int0105-file3.mp4","int0105-file4.png","int0105-file5.mp4","Enhanced Tactile Feedback Using a Mobile and a Wearable Device","Experience a amazing tactile feedbacks using mobile and wearable devices!","","116","Harmonious Haptics: Enhanced Tactile Feedback Using a Mobile and a Wearable Device","Sungjae Hwang","Junghyeon Gim","earlyrelease","","FormatComplete","","","","","","","Mar  2 22:02",""
"int108","A","Low-Fidelity Fabrication: Speeding up Design Iteration of 3D Objects","Stefanie","Mueller","stefanie.mueller@student.hpi.uni-potsdam.de","int0108-paper.pdf","4","letter","","","Stefanie Mueller, Dustin Beyer, Tobias Mohr, Serafima Gurevich, Alexander Teibrich, Lisa Pfisterer, Kerstin Guenther, Johannes Frohnhofen, Hsiang-Ting Chen, Patrick Baudisch, Sangha Im, François V Guimbretière","stefanie.mueller@student.hpi.uni-potsdam.de, dustin.beyer@student.hpi.uni-potsdam.de, tobias.mohr@student.hpi.uni-potsdam.de, Serafima.Gurevich@hpi.uni-potsdam.de, alexander.teibrich@student.hpi.uni-potsdam.de, lisa.pfisterer@student.hpi.uni-potsdam.de, kerstin.guenther@student.hpi.uni-potsdam.de, johannes.frohnhofen@student.hpi.uni-potsdam.de, ht.timchen@gmail.com, patrick.baudisch@hpi.uni-potsdam.de, si237@cornell.edu, francois@cs.cornell.edu","21882","Stefanie","","Mueller","stefanie.mueller@student.hpi.uni-potsdam.de","","Hasso Plattner Institute","Potsdam","","Germany","","","","","","45711","Dustin","","Beyer","dustin.beyer@student.hpi.uni-potsdam.de","","Hasso Plattner Institute","Potsdam","","Germany","","","","","","37113","Tobias","","Mohr","tobias.mohr@student.hpi.uni-potsdam.de","","Hasso Plattner Institute","Potsdam","","Germany","","","","","","42826","Serafima","","Gurevich","Serafima.Gurevich@hpi.uni-potsdam.de","","Hasso Plattner Institute","Potsdam","","Germany","","","","","","43103","Alexander","","Teibrich","alexander.teibrich@student.hpi.uni-potsdam.de","","Hasso Plattner Institute","Potsdam","","Germany","","","","","","43108","Lisa","","Pfisterer","lisa.pfisterer@student.hpi.uni-potsdam.de","","Hasso Plattner Institute","Potsdam","","Germany","","","","","","37114","Kerstin","","Guenther","kerstin.guenther@student.hpi.uni-potsdam.de","","Hasso Plattner Institute","Potsdam","","Germany","","","","","","37115","Johannes","","Frohnhofen","johannes.frohnhofen@student.hpi.uni-potsdam.de","","Hasso Plattner Institute","Potsdam","","Germany","","","","","","27038","Hsiang-Ting","","Chen","ht.timchen@gmail.com","","Hasso Plattner Institute","Potsdam","","Germany","","","","","","1437","Patrick","","Baudisch","patrick.baudisch@hpi.uni-potsdam.de","","Hasso Plattner Institute","Potsdam","","Germany","","","","","","43105","Sangha","","Im","si237@cornell.edu","Computer Science","Cornell University","Ithaca","NY","United States","","","","","","1483","François","V","Guimbretière","francois@cs.cornell.edu","","Cornell University","Ithaca","NY","United States","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Low-fidelity fabrication systems speed up rapid prototyping by printing intermediate versions of a prototype as fast, low-fidelity previews. Only the final version is fabricated as a full high-fidelity 3D print. This allows designers to iterate more quickly—achieving a better design in less time.  \  \ Depending on what is currently being tested, low-fidelity fabrication is implemented in different ways: (1) faBrickator allows for a modular approach by substituting sub-volumes of the 3D model with building blocks. (2) WirePrint allows for quickly testing the shape of an object, such as the ergonomic fit, by printing wireframe structures. (3) Platener preserves the technical function by substituting 3D print with laser-cut plates of the same size and thickness.  \  \ At our CHI’15 interactivity booth, we give a combined live demo of all three low-fidelity fabrication systems—putting special focus on our new low-fidelity fabrication system Platener (paper at CHI’15).  \ ","Stefanie Mueller","stefanie.mueller@student.hpi.uni-potsdam.de","1. Beyer, D., Gurevich, S., Mueller, S., Chen, T., Baudisch, T. Platener: Low-Fidelity Fabrication of 3D Objects by Substituting 3D Print with Laser-Cut Plates. Proc. CHI’15. \ 2. Mueller, S., Im, S., Gurevich, S., Teibrich, A., Pfisterer, L., Guimbretière, F. WirePrint: Fast 3D Printed Previews. Proc. UIST’14, 273-280. \ 3. Mueller, S., Mohr, T., Guenther, K., Frohnhofen, J., Baudisch, P. faBrickation: Fast Fabrication of Functional Objects By Integrating Construction Kit Building Blocks. Proc. CHI’14, 3827-3834. \ ","rapid prototyping; design iteration; prototyping, 3D printing, laser-cutting, personal fabrication","H.5.2","int0108-file1.docx","int0108-file2.jpg","int0108-file3.mp4","int0108-file4.jpg","int0108-file5.mp4","Low-fidelity fabrication systems speed up rapid prototyping by printing intermediate versions of a prototype as fast low-fidelity previews. Only the final version is fabricated as a full high-fidelity 3D print. ","Low-fidelity fabrication systems speed up rapid prototyping by printing intermediate versions of a prototype as fast low-fidelity previews. Only the final version is fabricated as a full high-fidelity 3D print. ","","124","Low-Fidelity Fabrication: Speeding up Design Iteration of 3D Objects","Stefanie Mueller","Patrick Baudisch","earlyrelease","","FormatComplete","","","","","","","Feb 10 11:04",""
"int109","A","PaperPulse: An Integrated Approach to Fabricating Interactive Paper","Kashyap","Todi","kashyap.todi@uhasselt.be","int0109-paper.pdf","4","letter","Times-Roman","","Raf Ramakers, Kashyap Todi, Kris Luyten","raf.ramakers@gmail.com, kashyap.todi@uhasselt.be, kris.luyten@uhasselt.be","27274","Raf","","Ramakers","raf.ramakers@gmail.com","Expertise Centre for Digital Media","Hasselt University - tUL - iMinds","Diepenbeek","","Belgium","","","","","","40261","Kashyap","","Todi","kashyap.todi@uhasselt.be","Expertise Centre for Digital Media","Hasselt University - tUL - iMinds","Diepenbeek","","Belgium","","","","","","2807","Kris","","Luyten","kris.luyten@uhasselt.be","Expertise Centre for Digital Media","Hasselt University - tUL - iMinds","Diepenbeek","","Belgium","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","We present PaperPulse, a design and fabrication approach that enables designers to produce standalone interactive paper artifacts by augmenting them with electronics. With PaperPulse, users overlay visual designs with widgets provided in the design tool. PaperPulse provides three families of widgets, designed for smooth integration with paper, for a total of 20 different interactive components. We also contribute a demonstration and recording approach, Pulsation, that allows specifying interaction logic. Using the final design and the recorded Pulsation logic, PaperPulse generates layered electronic circuit designs, and code that can be deployed on a microcontroller. By following automatically generated assembly instructions, designers can seamlessly integrate the microcontroller and widgets in the final paper artifact.","Kashyap Todi","kashyap.todi@uhasselt.be","1. Kawahara, Y., Hodges, S., Cook, B. S., Zhang, C., and Abowd, G. D. Instant inkjet circuits: Lab-based inkjet printing to support rapid prototyping of ubicomp devices. In Proc. UbiComp ’13. \ 2. Qi, J., and Buechley, L. Electronic popables: Exploring paper-based computing through an interactive pop-up book. In Proc. TEI ’10. \ 3. Savage, V., Zhang, X., and Hartmann, B. Midas: Fabricating custom capacitive touch sensors to prototype interactive objects. In Proc. UIST ’12. \ 4. Shorter, M., Rogers, J., and McGhee, J. Enhancing everyday paper interactions with paper circuits. In Proc. DIS ’14. \ ","Paper Electronics; Paper-crafts; Fabrication; Design Tools; PBD; Tangible Interfaces","H.5.2","int0109-file1.zip","int0109-file2.jpg","int0109-file3.mp4","int0109-file4.png","int0109-file5.mp4","An interactive paper game made using PaperPulse. The sheet has a pull-chain switch, blinking lights, and a button.","Experience the entire process of designing, printing, and assembling standalone interactive paper artefacts using PaperPulse. No programming or electronics skills required!","","109","PaperPulse","Raf Ramakers","Kashyap Todi","earlyrelease","","FormatComplete","","","","","","","Feb 16 11:13",""
"int110","A","Level-Ups: Motorized Stilts that Simulate Stair Steps in Virtual Reality","Dominik","Schmidt","dominik.schmidt@hpi.uni-potsdam.de","int0110-paper.pdf","4","letter","","","Dominik Schmidt, Robert Kovacs, Vikram Mehta, Udayan Umapathi, Sven Köhler, Lung-Pan Cheng, Patrick Baudisch","dominik.schmidt@hpi.uni-potsdam.de, robert.kovacs@hpi.de, vmehta@mpi-inf.mpg.de, udayan.umapathi@hpi.de, sven.koehler@student.hpi.uni-potsdam.de, lung-pan.cheng@hpi.uni-potsdam.de, patrick.baudisch@hpi.uni-potsdam.de","28799","Dominik","","Schmidt","dominik.schmidt@hpi.uni-potsdam.de","","Hasso Plattner Institute","Potsdam","","Germany","","","","","","45709","Robert","","Kovacs","robert.kovacs@hpi.de","","Hasso Plattner Institute","Potsdam","","Germany","","","","","","38820","Vikram","","Mehta","vmehta@mpi-inf.mpg.de","","Hasso Plattner Institute","Potsdam","","Germany","","","","","","45708","Udayan","","Umapathi","udayan.umapathi@hpi.de","","Hasso Plattner Institute","Potsdam","","Germany","","","","","","28803","Sven","","Köhler","sven.koehler@student.hpi.uni-potsdam.de","","Hasso Plattner Institute","Potsdam","","Germany","","","","","","17012","Lung-Pan","","Cheng","lung-pan.cheng@hpi.uni-potsdam.de","","Hasso Plattner Institute","Potsdam","","Germany","","","","","","1437","Patrick","","Baudisch","patrick.baudisch@hpi.uni-potsdam.de","","Hasso Plattner Institute","Potsdam","","Germany","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","We present “Level-Ups”, computer-controlled stilts that allow virtual reality users to experience walking up and down steps. Each Level-Up unit is a self-contained device worn like a boot. Its main functional element is a vertical actuation mechanism mounted to the bottom of the boot that extends vertically. Unlike traditional solutions that are integrated with locomotion devices, Level-Ups allow users to walk around freely (“real-walking”). We present Level-Ups in a demo environment based on a head-mounted display, optical motion capture, and integrated with a game engine.","Rob Kovacs","robert.kovacs@hpi.de","1. Begault, D.R. 3-D Sound for Virtual Reality and Multimedia. AP Professional, 1994. \ 2. Cheng, L.P., Lühne, P., Lopes, P., et al. Haptic Turk: a Motion Platform Based on People. In Proc. CHI’14, 3463–3472. \ 3. Iwata, H., Yano, H., and Nakaizumi, F. Gait Master: a Versatile Locomotion Interface for Uneven Virtual Terrain. Virtual Reality, (2001). \ 4. Schmidt, H., Hesse, S., Bernhardt, R., et al. HapticWalker—a Novel Haptic Foot Device. ACM Transactions on Applied Perception 2, 2005, 166– 180. \ 5. Sutherland, I.E. A Head-Mounted Three Dimensional Display. In Proc. AFIPS’68, 757/764. \ 6. Ward, M., Azuma, R., Bennett, R., et al. A Demonstrated Optical Tracker with Scalable Work Area for Head-Mounted Display Systems. In Proc. Symp. on Interact. 3D Graph.’92, 43–52.","Virtual Reality; Real-Walking; Head-Mounted Display","H.5.2","int0110-file1.docx","int0110-file2.jpg","int0110-file3.mp4","int0110-file4.jpg","int0110-file5.mp4","Level-Ups are motorized stilts that allow virtual reality users to experience walking up and down steps. The main benefit of our approach is its applicability to real-walking in virtual worlds.","Level-Ups, motorized stilts that extend and collapse controlled by the computer, allow you to experience walking up and down steps in virtual worlds.","","202","Level-Ups: Motorized Stilts that Simulate Stair Steps in Virtual Reality","Rob Kovacs","Dominik Schmidt","earlyrelease","","FormatComplete","","","","","","","Feb  5 15:56",""
"int115","A","Designing Engaging Data in Communities","Alex","Taylor","ast@microsoft.com","int0115-paper.pdf","4","letter","","","Tim Regan, David Sweeney, John Helmes, Vasillis Vlachokyriakos, Siân Lindley, Alex Taylor","timregan@microsoft.com, dsweeney@microsoft.com, johnhelmes@gmail.com, v.vlachokyriakos@newcastle.ac.uk, sianl@microsoft.com, ast@microsoft.com","2003","Tim","","Regan","timregan@microsoft.com","","Microsoft Research","Cambridge","","United Kingdom","","","","","","42147","David","","Sweeney","dsweeney@microsoft.com","Microsoft Research Cambridge","Microsoft","Cambridge","","United Kingdom","","","","","","12221","John","","Helmes","johnhelmes@gmail.com","","Microsoft Research ","Cambridge","Cambridgeshire","United Kingdom","","","","","","23352","Vasillis","","Vlachokyriakos","v.vlachokyriakos@newcastle.ac.uk","Culture Lab","Newcastle University","Newcastle upon Tyne","","United Kingdom","","","","","","2278","Siân","","Lindley","sianl@microsoft.com","","Microsoft Research","Cambridge","Cambridgeshire","United Kingdom","","","","","","2037","Alex","","Taylor","ast@microsoft.com","","Microsoft Research","Cambridge","","United Kingdom","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","We present two sets of ‘data technologies’ that we have designed to collect and display local data, both derived from our engagement with a community. The first, Bull-frog, is a bespoke voting device. The second, a series of physical charts, respond to the increasing sophistica-tion of data visualisations by making playful use of pie charts and bar graphs, reimagining them in mechanical forms that are compelling but easily readable.","Alex Taylor","ast@microsoft.com","1. Air Quality Eggs. http://airqualityegg.com. \ 2. Jansen, J., Dragicevic, P., and Fekete, J. 2013. Evaluating the efficiency of physical visualizations. In Proc. CHI '13. ACM Press (2013), 2593-2602. \ 3. Prensky, M. H. sapiens digital: from digital immigrants and digital natives to digital wisdom. Innovate 5 (3, 2009). \ 4. Taylor, A., Lindley, S., Regan, T., Grainger, L., Sweeney, D., Vlachokyriakos, V., Lingel, J. Data-inplace: thinking through the relations between data and community. Submitted to CHI 2015. \ 5. Vlachokyriakos, V., Comber, R., Ladha, K., Taylor, N., Dunphy, P., McCorry, P. and Olivier, P. PosterVote: expanding the action repertoire for local political activism. In Proc. DIS 2014, ACM Press (2014), 795-804. Figure 3. Charts on display at Tenison Road street party. Devices in use at Victoria and Albert Museum, London Design Festival. \ ","Data; data-in-place; information visualization; tangible; spectacle; voting; digital civics; community.","H.5.m.","int0115-file1.doc","int0115-file2.jpeg","int0115-file3.m4v","int0115-file4.jpg","int0115-file5.mp4","Graphic for Tenison Road project.","Exhibit showcasing data tools designed for a community project in Cambridge (UK), on Tenison Road. The tools aim to materially engage community in the processes of data production and use.","","110","Designing Engaging Data in  Communities","David Sweeney","Alex Taylor","earlyrelease","","FormatComplete","","","","","","","Feb  5 16:04",""
"int116","A","The EmotiveModeler: An Emotive Form Design CAD Tool","Philippa","Mothersill","pip@mit.edu","int0116-paper.pdf","4","letter","","","Philippa Mothersill, V. Michael Bove Jr.","pip@mit.edu, vmb@media.mit.edu","45579","Philippa","","Mothersill","pip@mit.edu","MIT Media Lab","MIT","Cambridge","Massachusetts","USA","","","","","","12310","V. Michael","","Bove Jr.","vmb@media.mit.edu","Media Lab","MIT","Cambridge","Massachusetts","United States","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Whether or not we are experts in the design language of objects, we have an unconscious understanding of the emotional character of their forms.  The EmotiveModeler integrates knowledge about our emotive perception of shapes into a CAD tool that uses descriptive adjectives as an input to aid designers in creating objects that can communicate emotive character.  Through inputting words into the EmotiveModeler UI in the Rhinoceros 3D modeling software, both expert and novice designers can manipulate the design of a bottle to express emotive character through its form.","Philippa Mothersill","pip@mit.edu","1. Broadbent, G. H. (1966) Creativity. In The design method (pp111-120) Butterworths \ 2. Chaudhuri, S. et al. (2013, October). Attribit: content creation with semantic attributes. In Proceedings of the 26th annual ACM symposium on User interface software and technology, ACM. \ 3. Collier, G. L. (1996). Affective synesthesia: Extracting emotion space from simple perceptual stimuli. Motivation and Emotion, 20(1), 1-32. \ 4. Gorno, R. & Colombo, S. (June 2011). Attributing intended character to products through their formal features. In Proceedings of the 2011 Conference on Designing Pleasurable Products and Interfaces ACM. \ 5. Gregory, S. A. (1966) Design and the Design Method. In The design method. (pp.3-10) Butterworths. \ 6. Isbister, K., Höök, K., Laaksolahti, J., & Sharp, M. (2007). The sensual evaluation instrument: Developing a trans-cultural self-report measure of affect International journal of human-computer studies, 65(4), 315-328 \ 7. Jacobs, J., & Buechley, L. (2013, April). Codeable objects: computational design and digital fabrication for novice programmers. In Proceedings of the 2013 ACM annual conference on Human factors in computing systems (pp. 1589-1598). ACM. \ 8. Janlert, L. E., & Stolterman, E. (1997). The character of things. Design Studies, 18(3), 297-314. \ 9. Kalogerakis, E., Chaudhuri, S., Koller, D., & Koltun, V. (2012). A probabilistic model for component-based shape synthesis. ACM Transactions on Graphics (TOG) \ 10. Krippendorff, K. & R. Butter (1984). Product Semantics: Exploring the Symbolic Qualities of Form. Innovation Spring 1984. pp. 4-9. \ 11. Lesot. M-J., Bouchard, C., Detynie, M., Omhover, JF. Product Shape And Emotional Design: an application to perfume bottles In Proceedings of International Conference on Kansei Engineering and Emotion Research (KEER) 2010 \ 12. McCrory, R.J. (1966) The Design Method in Practice In The design method (pp.11-18) Butterworths. \ 13. Meier, M. Easily Create Graphical User Interfaces in Rhino Python. Retrieved July 29, 2014, from http://mkmra2.blogspot.com/2012/12/creatinggraphical-user-interfaces-with.html \ 14. Mohammad, S. M., & Turney, P. D. (2013). NRC Emotion Lexicon. \ 15. Plutchik, R. (1991) The emotions. University Press of America. \ 16. Poffenberger, A. & Barrows, B. (1924) The Feeling Value of Lines Journal of Applied Psychology, 8(2), 187 \ 17. Rhinoceros 3D. In Wikipedia. Retrieved July 29, 2014, from http://en.wikipedia.org/wiki/Rhinoceros_3D \ 18. Russell, James A. A circumplex model of affect. Journal of personality and social psychology 39.6 (1980): 1161-1178 \ 19. Smyth, S. N., & Wallace, D. R. (2000, September). Towards the synthesis of aesthetic product form. In Proc. DETC2000/DTM-14554, ASME, New York. \ 20. Szewczyk, J. (2003). Difficulties with the novices' comprehension of the computer-aided design (CAD) interface: Understanding visual representations of CAD tools. Journal of Engineering Design, 14(2), 169-185. \ 21. Thomas, F. & Johnston, O. (1995). The illusion of life: Disney animation New York: Hyperion","Design; emotion; semantically-driven CAD tool","D.2.2; J.6","int0116-file1.docx","","int0116-file3.mp4","int0116-file4.png","int0116-file5.mp4","","Discover your inner expressive designer while using the EmotiveModeler CAD tool to manipulate the character of 3D forms using only emotive adjectives.","","127","The EmotiveModeler: An Emotive Form Design CAD Tool","Philippa Mothersill","V. Michael Bove Jr.","earlyrelease","","FormatComplete","","MIT Media Lab consortium","","","","","Feb 16 16:41",""
"int117","A","Remnance of Form: Interactive Narratives through Unexpected Behaviors of a Shadow","Sang-won","Leigh","sangwon@mit.edu","int0117-paper.pdf","4","letter","","","Sang-won Leigh, Asta Roseway, Ann Paradiso, Pattie Maes","sangwon@mit.edu, astar@microsoft.com, annpar@microsoft.com, pattie@media.mit.edu","36740","Sang-won","","Leigh","sangwon@mit.edu","Media Lab","MIT","Cambridge","Massachusetts","United States","","","","","","16421","Asta","","Roseway","astar@microsoft.com","","Microsoft Research","Redmond","Washington","United States","","","","","","47596","Ann","","Paradiso","annpar@microsoft.com","","Microsoft Research","Redmond","Washington","United States","","","","","","3707","Pattie","","Maes","pattie@media.mit.edu","","MIT Media Lab","Cambridge","Massachusetts","United States","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Remnance of Form is an interactive installation that explores the dynamic tension between an object and its shadow. By fusing light, projection, and motion technologies, the shadow can now detach itself from its former role. This creates a new narrative that challenges our perception of reality, what’s real and what’s not. Through several playful vignettes, the shadow interacts with viewers’ presence, body posture, and their manipulation of the light source creating the shadow.","Sang-won Leigh","sangwon@mit.edu","1. http://dpt.co/en/parade-2/ \ 2. http://joonmoon.net/Augmented-Shadow/ \ 3. http://milk.co/treachery/ \ 4. http://www.kumiyamashita.com/light-and-shadow \ 5. http://www.majapetric.com/portfolio/shadowinverted/ \ 6. http://www.snibbe.com/projects/interactive/shado wbag/ \ 7. http://www.timnobleandsuewebster.com/artwerks. html \ 8. Feiner, S., MacIntyre, B., Haupt, M., Solomon, M. (1993). Windows on the world: 2D windows for 3D augmented reality. ACM UIST. p. 145-155 \ 9. Ishii, H. and Ullmer, B. Tangible Bits. Towards seamless interfaces between people, bits and atoms. In Proc.: CHI’97, ACM Press (1997), 234–241 \ 10. Underkoffler, J. and Ishii, H. Urp: a luminoustangible workbench for urban planning and design. Proceedings of the SIGCHI conference on Human factors in computing systems: the CHI is the limit, ACM (1999), 386-393 \ 11. Weiser, M., The computer for the twenty-first century. Scientific American. v265 i3. 94-104 Figure 6. Interacting with the installation. \ ","Shadow; Augmented Reality; Interactive Art Installation","H.5.m.","int0117-file1.doc","int0117-file2.jpg","int0117-file3.mp4","int0117-file4.jpg","int0117-file5.mp4","Remnance of Form","Interactive Narratives through Unexpected Behaviors of a Shadow","","209","Remnance of Form","Sang-won Leigh","Asta Roseway","earlyrelease","","FormatComplete","","","","","","","Feb 15 21:54",""
"int123","A","Wearable Devices for Enhancing Communications and Activities between the Blind and Ordinary People through a Waltz","Yoonji","Song","yoonji816@gmail.com","int0123-paper.pdf","4","letter","","","Yoonji Song, Jiye Kim","yoonji816@gmail.com, mingle14@naver.com","47634","Yoonji","","Song","yoonji816@gmail.com","Hongik University","Digital media design","Sejong","Chungcheongnam-do","Korea, Republic of","","","","","","47914","Jiye","","Kim","mingle14@naver.com","Hongik University","Digital media design","Sejong","Chungcheongnam-do","Korea, Republic of","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","This research has begun from recognizing a problem as to activities that arises by limitations of visual information for the blind. Mingle is a wearable device as a communication medium through senses of hearing and touch which helps the blind and ordinary people who have no visual impairment easily and intuitively learn a waltz. The entire device operation coupled with a waltz basic step comprises of 4 phases of a step, basic step, box step and an advanced box step linked to the song. In terms of a modality, hearing information informs of the voice information as to steps and tactual information informs of the directions of the steps that vibration information located in each device teaches. All the information are communicated to the blind and ordinary people in the same way and through this, the blind and ordinary people can learn the dance without any visual reliance.","Yoonji Song","yoonji816@gmail.com","1. Dong Cheon Kang (2005), Universal Design, Package design research according to a strategy - based on the package design with a consideration for the blind -, pp 60-62 \ 2. Deok Joong Yoon (2003), Factors a ecting the participation to leisure activities of the blind, pp.12-14 \ 3. Arduino <http:// www.arduino.cc> \ ","Waltz; Dance; Blind; Communication; Wearable Device;  Installationl; Branding","H.5.1; H.5.2; I.2.6","int0123-file1.docx","","int0123-file3.avi","int0123-file4.png","","mingle","Mingle is a wearable device as a communication medium through senses of hearing and touch which helps the blind and non-blind easily and intuitively learn a waltz. ","","204","Mingle","Yoonji Song","Jiye Kim","earlyrelease","","FormatComplete","","","","","","","Feb  4 12:38",""
"int124","A","EdiPulse: Turning Physical Activity Into Chocolates","Rohit Ashok","Khot","rohit.a.khot@gmail.com","int0124-paper.pdf","4","letter","","","Rohit Ashok Khot, Ryan Pennings, Florian 'Floyd' Mueller","rohit.a.khot@gmail.com, s3378565@student.rmit.edu.au, floyd@floydmueller.com","10580","Rohit Ashok","","Khot","rohit.a.khot@gmail.com","Exertion games lab","RMIT University","Melbourne","Victoria","Australia","","","","","","47644","Ryan","","Pennings","s3378565@student.rmit.edu.au","Industrial Design","RMIT University","Melbourne","Victoria","Australia","","","","","","2060","Florian","'Floyd'","Mueller","floyd@floydmueller.com","Exertion Games Lab","RMIT University","Melbourne","Victoria","Australia","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","We present EdiPulse that creates 3D printed chocolates displaying cheerful messages using the heart rate data of physical activity. Our work expands the view on representing physical activity data through the use of edible materials such as chocolates, which additionally serves as a hedonic reward for doing the physical activity. Ultimately, with this work, we aim to inspire and guide design thinking on food printing, which we believe opens up new interaction possibilities to support the physical activity experience.","Rohit Ashok Khot","rohit.a.khot@gmail.com","1. Choc Edge, http://chocedge.com. \ 2. Choi, J., Linehan, C., Comber, R. and McCarthy J. Food for thought: designing for critical reflection on food practices. In Proc. DIS '12, ACM, 793-794. \ 3. Comber, R., Choi, J., Hoonhout, J. and O'hara, K. Editorial: Designing for human-food interaction: An introduction to the special issue on 'food and interaction design', IJHCS, 2014, 72(2), 181-184. \ 4. Consolvo, S., McDonald, D. and Landay, J. Theorydriven design strategies for technologies that support behavior change in everyday life. In Proc. CHI’09, ACM Press (2009), 405-414. \ 5. Fukuchi, K., Jo, K., Tomiyama, A., and Takao, S. Laser cooking: a novel culinary technique for dry heating using a laser cutter and vision technology. In Proc. workshop on Multimedia for cooking and eating activities (CEA '12). ACM, 55-58. \ 6. Khot, R., Hjorth, L. and Mueller, F. Understanding physical activity through 3D printed material artifacts. In Proc. CHI '14, ACM (2014), 3835-3844. \ 7. Li, I., Dey, A., and Forlizzi, J. Understanding my data, myself: supporting self-reflection with ubicomp technologies. In Proc. UbiComp '11, ACM Press (2011), 405-414. \ 8. Lin, J., Mamykina, L., Lindtner, S., Delajoux, G. and Strub, H. Fish’n’Steps: Encouraging physical activity with an interactive computer game. In Proc. Ubicomp’06, Springer (2006), 261-278. \ 9. Lupton, D. Food, the Body and the Self. London: SAGE Publications Ltd., 1998. \ 10. OpenJSCAD, http://openjscad.org. \ 11. Polar heart rate monitors. http://polar.com. \ 12. Qkies; http://qkies.de. \ 13. Schoning, J., Rogers, Y., and Kruger, A. Digitally Enhanced Food. IEEE Pervasive Computing, 2012, 4-6. \ 14. Spence, C. and Piqueras-Fiszman, B. Technology at the dining table. Flavour, 2013, 2 (1), 16. \ 15. Wei, J., Ma, X., and Zhao, S. Food Messaging: Using an Edible Medium for Social Messaging. In Proc. CHI’14, ACM (2014), 2873-2882. \ ","Physical activity; edible representations; food HCI","H.5.m","int0124-file1.doc","int0124-file2.jpg","","int0124-file4.jpg","int0124-file5.mp4","EdiPulse turns heart rate data into cheerful messages printed in  chocolate.","Sweat was never so Sweet. Come experience the journey from Sweat to Sweet.","","125","EdiPulse: Turning Physical Activity Into Chocolates","Rohit Ashok Khot","Florian Mueller","earlyrelease","","FormatComplete","","","","","","","Feb 11 20:05",""
"int128","A","ListenTree: Audio-Haptic Display In The Natural Environment","Edwina","Portocarrero","edwina@media.mit.edu","int0128-paper.pdf","4","letter","","","Edwina Portocarrero, Gershon Dublon, Joseph Paradiso, V. Michael Bove Jr.","edwina@media.mit.edu, gershon@media.mit.edu, joep@media.mit.edu, vmb@media.mit.edu","15471","Edwina","","Portocarrero","edwina@media.mit.edu","MIT","MIT Media Laboratory","Cambridge","Massachusetts","United States","","","","","","24787","Gershon","","Dublon","gershon@media.mit.edu","","Massachusetts Institute of Technology","Cambridge","MA","USA","","","","","","4140","Joseph","","Paradiso","joep@media.mit.edu","MIT Media Lab","Massachusetts Institute of Technology","Cambridge","Massachusetts","United States","","","","","","12310","V. Michael","","Bove Jr.","vmb@media.mit.edu","Media Lab","MIT","Cambridge","Massachusetts","United States","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","In this paper, we present ListenTree, an audio-haptic display embedded in the natural environment. A visitor to our installation notices a faint sound appearing to emerge from a tree, and might feel a slight vibration under their feet as they approach. By resting their head against the tree, they are able to hear sound through bone conduction. To create this effect, an audio exciter transducer is weatherproofed and attached to the tree trunk underground, transforming the tree into a living speaker that channels audio through its branches. Any source of sound can be played through the tree, including live audio or pre-recorded tracks. For example, we used the ListenTree to display live streaming sound from an outdoor ecological monitoring sensor network, bringing an urban audience into contact with a faraway wetland. Our intervention is motivated by a need for forms of display that fade into the background, inviting attention rather than requiring it. ListenTree points to a future where digital information might become a seamless part of the physical world.","Edwina Portocarrero","edwina@media.mit.edu","1. M. Weiser and J. S. Brown, “Designing calm technology,” PowerGrid Journal, vol. 1, no. 1, pp. 75– 85, 1996. \ 2. L. Anderson, “The handphone table,” https://www. moma.org/pdfs/docs/pressarchives/5652/releases/ MO MA 1978 0088 81.pdf, August 1978. \ 3. M. Kison, “touched echo,” http://www.markuskison.de/ #touched echo, December 2007. \ 4. W. Jacob, “Floor, waves and signs, silent mixer,” http:// wendyjacob.net/?page id=133, 2009-2013[5] I. Poupyrev, P. Schoessler, J. Loh, and M. Sato, “Botanicus in- teracticus: Interactive plants technology,” in ACM SIGGRAPH 2012 Emerging Technologies, 2012. \ 5. I. Poupyrev, P. Schoessler, J. Loh, and M. Sato, “Botanicus in- teracticus: Interactive plants technology,” in ACM SIGGRAPH 2012 Emerging Technologies, 2012. \ ","Audio-Haptic displays; calm technologies, natural environment","H.5.m. Information interfaces and presentation.","int0128-file1.doc","","","int0128-file4.jpg","","","Touch and lean your head against the tree to hear \ sound streaming live from a remote place. Vibrations are audible through bone conduction, turning the tree into a telepresence portal.","","301","ListenTree: Audio-Haptic Display In The Natural Environment ","Edwina Portocarrero","Gershon Dublon","earlyrelease","","FormatComplete","","","","","","","Feb  6 09:03",""
"int131","A","Know Yourself: Self-portrait with Emotion Expressed in the EEG Data","Suyeon","Kim","kimsuyeon0@gmail.com","int0131-paper.pdf","4","letter","","","Hyo-jin Kim, Su-yeon Kim","hyonee1031@gmail.com, kimsuyeon0@gmail.com","47700","Hyo-jin","","Kim","hyonee1031@gmail.com","School of Design & Media","Hongik University, College of Design & Art","Jochiwon-eup","Sejong","Korea","","","","","","47700","Su-yeon","","Kim","kimsuyeon0@gmail.com","School of Design & Media","Hongik University, College of Design & Art","Jochiwon-eup","Sejong","Korea","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Self-portrait was painted for the reflection of the inner face of the old days, not only just draw a face of the human beings. So self-portrait is re-interpreted in various ways to adapt the new media platforms that is caused by the development of the media, and different to represent. In this study, our project - “Know yourself"" is not only tried to make an opportunity to express an external description, but also reflected them to draw a self-portrait for using brainwave that include one’s feelings. At first, ""Know yourself” is starting to make a question to the audience to see or not to see. It is a question that one's face reflected in a mirror is spoken real emotion and respected all about them. The technology of our project to draw the self-portrait is a linking methods use an 'EEG Analysis algorithm' express the face of the 'EEG headset' and face recognition processing solutions. We don’t have many chances to remind our feelings because of the many circumstances that surround to you or change suddenly. We sincerely want that user makes an answer to use our self-inner portrait project.","Suyeon Kim","kimsuyeon0@gmail.com","ERROR: Sorry, but we could not find the bibliography in your PDF file.  Please entry it manually. \ ","Design; Media art; Self- portrait; Brainwave; BCI;","I.2.2","int0131-file1.docx","int0131-file2.jpg","","int0131-file4.jpg","","Know yourself (Interactive self portrait used by the brainwave)","Draw your self-portrait by using brainwave that include your feelings!","","205","Know yourself (Interactive self portrait used by the brainwave)","Suyeon Kim","Hyo-jin Kim","earlyrelease","","FormatComplete","","","","","","","Feb  9 09:25",""
"int132","A","Researcher: A Reading Application Helping the Flow of Research in Tablet and Mobile Phone","Minjeong","Kang","mjmiso@gmail.com","int0132-paper.pdf","4","letter","","","Minjeong Kang, Juhyun Eune","mjmiso@snu.ac.kr, jheune@snu.ac.kr","47702","Minjeong","","Kang","mjmiso@snu.ac.kr","Dept. of Crafts and Design","Seoul National University","Seoul","","Korea","","","","","","30944","Juhyun","","Eune","jheune@snu.ac.kr","Intermedia Lab","Seoul National University","Seoul","","Korea, Republic of","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","With a growing digital environment, a huge quantity of digital content is being created and distributed quickly. Therefore, people from academia are under pressure to create and study such content. In addition, there are a number of reading applications supporting different functions and diverse platforms, which distract the flow of research. To solve this problem, we created a prototype reading app, Researcher, for the tablet PC and mobile phone, which helps the flow of research by providing cooperation among platforms, seamlessly circulating between consumption and creation of contents, prioritizing contents by context, and holding attention by multimodal input. We conducted an in-depth interview and survey to verify the effectiveness of the features and to find out the appropriate modality of input for the flow of research.","Minjeong Kang","mjmiso@gmail.com","WARNING: Reference 5 is very long.  Please check it. \  \ 1. Kang, M., & Eune, J. (2011). Design Guide for the Reinforcement of a Seamless Cross-Platform Reading Experience, Journal of Korean Society Design Science, 24(4), 227-238 \ 2. Kang, M., & Eune, J. (2013). A Study on a Digital Reading Ecosystem based on Content for Flow Improvement, Journal of Korean Society Design Science, 26 (3), 177-194. \ 3. Kim, M. (2000). A Study on the Extension of Media for Reading in Information-Oriented Society, Korea reading association, 5, 65-81 \ 4. Tenopir, C., King, D. W., Edwards, S., & Wu, L. (2009, January). Electronic journals and changes in scholarly article seeking and reading patterns. In Aslib proceedings (Vol. 61, No. 1, pp. 5-32). Emerald Group Publishing Limited. \ 5. Walsh, M. (2007). Reading visual and multimodal texts: how is ‘reading’ different. Australian Journal of Language and Literacy, 29(1), 24-37. ","Digital Reading; Flow; Research; Mobile Application; Multimodal Input; Context Awareness","H.5.m. [Information interfaces and presentation]","int0132-file1.docx","int0132-file2.jpg","","int0132-file4.jpg","int0132-file5.mp4","Researcher is a reading application for research on a mobile phone and tablet, which helps the flow of research","Researcher is a mobile and tablet application for research that provides seamless change between consumption and creation of contents, prioritization of contents by context, and hold of attention by multimodal inputs.","","208","Researcher: A Reading Application Helping the Flow of Research in Tablet and Mobile Phone","Minjeong Kang","Minjeong Kang","earlyrelease","","FormatComplete","","","","","","","Feb 13 04:52",""
"int138","A","BandSense: Pressure-sensitive Multi-touch Interaction on a Wristband","Sungjae","Hwang","best@kaist.ac.kr","int0138-paper.pdf","4","letter","","","Youngseok Ahn, Sungjae Hwang, Hyungook Yoon, Jung-hee Ryu","youngseok.ahn@futureplay.co, sungjae.hwang@futureplay.co, hyungook.yoon@futureplay.co, junghee.ryu@futureplay.co","47799","Youngseok","","Ahn","youngseok.ahn@futureplay.co","Creative Lab","FuturePlay Inc.","Seoul","","Korea, Republic of","","","","","","47729","Sungjae","","Hwang","sungjae.hwang@futureplay.co","Creative Lab","FuturePlay Inc.","Seoul","","Korea, Republic of","","","","","","47800","Hyungook","","Yoon","hyungook.yoon@futureplay.co","Creative Lab","FuturePlay Inc.","Seoul","","Korea, Republic of","","","","","","47801","Jung-hee","","Ryu","junghee.ryu@futureplay.co","Creative Lab","FuturePlay Inc.","Seoul","","Korea, Republic of","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","In this paper, we propose a new interaction technique, called BandSense, which allows pressure-sensitive multi-touch interaction on a wristband. The proposed method provides users with a broader interaction area and higher input expressiveness, enabling a precision interaction with a less occlusion. To illustrate the potential of our approach, we present a series of example applications with several input vocabularies. We also describe the overall architecture of our system. We believe that our technique would greatly help users control a smartwatch easily and conveniently.","Sungjae Hwang","sungjae.hwang@futureplay.co","1. Chen, X. A., Grossman, T., Wigdor, D. J., and Firzmaurice, G. Duet: exploring joint interactions on a smart phone and a smart watch. In Proc. of CHI (2014), 159-168. \ 2. Funk, M., Sahami, A., Henze, N., and Schmidt, A. Using a touch-sensitive wristband for text entry on smart watches. In Proc. of CHI EA (2014), 2305-2310. \ 3. Harrison, C., and Hudson, S. Abracadabra: Wireless, High-precision, and Unpowered Finger Input for Very Small Mobile Devices. In Proc. of UIST (2009), 121-124. \ 4. Knibbe, J., Plasencia, D. M., Bainbridge, C., Chan, C. K., Wu, J., Cable, T., Munir, H., and Coyle, D. Extending interaction for smart watches: enabling bimanual around device control. In Proc. of CHI EA (2014), 1891-1896. \ 5. Miyaki, T., Rekimoto, J., GraspZoom: zooming and scrolling control model for single-handed mobile interaction, In Proc. of MobileHCI, (2009), 81-84 \ 6. Narayanaswami, C., and Raghunath, M. T. Application Design for a Smart Watch with a High Resolution Display. In Proc. of ISWC (2010), 7-14. \ 7. Oney, S., Harrison, C., Ogan, A., Wiese, J., Zoomboard: A Diminutive QWERTY Soft Keyboard Using Iterative Zooming for Ultra-Small Devices. In Proc. Of CHI (2013) \ 8. Xiao, R., Laput, G., and Harrison, C. Expanding the input expressivity of smartwatches with mechanical pan, twist, tilt and click. In Proc. of CHI (2014), 193-196. \ ","Wristband; smartwatch; pressure-sensitive input, multi-touch; resistive multi-touch; display occlusion; wearable devices; tangential force","H.5.2","int0138-file1.doc","int0138-file2.jpg","int0138-file3.mp4","int0138-file4.png","int0138-file5.mp4","BandSense: Pressure-sensitive Multi-touch Interaction on a Wristband ","Experience a new interaction technique on a Wristband!","","101","BandSense: Pressure-sensitive Multi-touch Interaction on a Wristband ","Youngseok Ahn","Sungjae Hwang","earlyrelease","","FormatComplete","","","","","","","Mar  9 19:28",""
"int139","A","Wearable Solution for Industrial Maintenance","Sam","Zheng","sam.zheng@gmail.com","int0139-paper.pdf","4","letter","","","Sam Zheng, Patrik Matos, Cedric Foucault, Siddharth Dasari, Meng Yuan, Stuart Goose","sam.zheng@gmail.com, patrik.matos@siemens.com, cedric.foucault@siemens.com, siddharth.dasari@siemens.com, yuanmeng@siemens.com, stuart.goose@siemens.com","6008","Sam","","Zheng","sam.zheng@gmail.com","","Siemens Corporate Research","Princeton","New Jersey","United States","","","","","","47215","Patrik","","Matos","patrik.matos@siemens.com","","Siemens Corporate Research","Princeton","New Jersey","United States","","","","","","47214","Cedric","","Foucault","cedric.foucault@siemens.com","","Siemens Corporate Research","Princeton","New Jersey","United States","","","","","","47216","Siddharth","","Dasari","siddharth.dasari@siemens.com","","Siemens Corporate Research","Princeton","New Jersey","United States","","","","","","47890","Meng","","Yuan","yuanmeng@siemens.com","","Siemens Corporate Research","Princeton","New Jersey","United States","","","","","","47218","Stuart","","Goose","stuart.goose@siemens.com","","Siemens Technology to Business","Berkeley","California","United States","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Wearable technology, such as Google Glass, offers potential benefits to engineers in industrial settings. We designed and developed a wearable solution for industrial maintenance, which 1) provides workflow guidance to the user, 2) supports hands-free operation, 3) allows the users to focus on their work, and 4) enables an efficient way for collaborating with a remote expert. The prototype, which was demonstrated at InnoTrans 2014, the largest international trade show for train technology, received positive feedback from many potential users and customers.","Sam Zheng","sam.zheng@gmail.com","1. Gartner Says Smartglasses Will Bring Innovation to Workplace Efficiency,2014. http://gtnr.it/1qqbXce. \ 2. Machinery Maintenance & Heavy Equipment Repair Services Market Research Report, 2014. http://bit.ly/1kVmv6N. \ 3. Zheng, X.S., et. al. Eye-wearable technology for machine maintenance: Effects of display position and hands-free operation. In Proc. CHI 2015. \ ","eye-wearable technology; machine maintenance; field service; workflow; wearable computing; smartglasses","H.5.m. Information interfaces and presentation (e.g., HCI): Miscellaneous","int0139-file1.doc","int0139-file2.jpg","","int0139-file4.png","int0139-file5.mp4","Following the workflow guidance by Google Glass, a field engineer is inspecting the HVAC system of a train. ","Wearable solution for industrial maintenance that provides workflow guidance, supports hands-free operation, allows the users to focus on their work, and enables an efficient way for remote collaboration. ","","120","Wearable Solution for Industrial Maintenance ","Sam Zheng","Patrik Matos","earlyrelease","","FormatComplete","","","","","","","Feb 16 17:55",""
"int143","A","Development of Realistic Digital Expression of Human Avatars through Pupillary Responses based on Heart Rate","Myoung Ju","Won","dnjsaudwn@naver.com","int0143-paper.pdf","4","letter","","","Myoung Ju Won, Sangin Park, SungTeac Hwang, Mincheol Whang","dnjsaudwn@naver.com, ini0630@naver.com, columstyle@naver.com, whang@smu.ac.kr","47698","Myoung Ju","","Won","dnjsaudwn@naver.com","Emotion Engineering","Sangmyung University","Seoul","Jongno-gu","Korea","","","","","","47741","Sangin","","Park","ini0630@naver.com","Department of Emotion Engineering","Sangmyung University","Seoul","","South Korea","","","","","","47742","SungTeac","","Hwang","columstyle@naver.com","Department of Emotion Engineering Lab","University of Sangmung","Seoul","Jongno-gu","South korea","","","","","","47745","Mincheol","","Whang","whang@smu.ac.kr","Media software","Sangmyung University","Seoul","Jongno-gu","Korea","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","The eyes of a virtual human avatar are the fundamental means for communicating language and affective feelings in a virtual environment. For this purpose, this study evaluates user's visual feeling according to the changes pupillary responses based on the heart rate for a representing a human avatar; this is considered as a new factor for representing a realistic avatar. We construct a human avatar in which the pupillary response is delivered real-time based on the heart rate. The results can be regarded as the basis for designing a realistic human avatar system by supporting a new visual realistic representing factor.","Myoung Ju Won","dnjsaudwn@naver.com","1. Barfield, W., and Hendrix, C. The effect of update rate on the scene of presence in virtual environment, Journal of virtual reality: research, development, applications (1995), 3-15. \ 2. Hendrix, C., and Barfield, W. Presence within virtual environments as a function of visual display parameters, Journal of presence: teleoperators and virtual environment (1996), 274-289. \ 3. Albrecht, I., Haber, J., and Seidel, H. Automatic Generation of Non-Verbal Facial Expressions from Speech, Proc. Computer Graphics Int’l (CGI ’02) (2002), 283-293. \ 4. Frith, U., and Frith, C. D. Development and neurophysiology of mentalizing, Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences (2003), 459-473. \ 5. Siegle, G. J., Steinhauer, S. R., Carter, C. S., Ramel, W., and Thase, M. E. Do the seconds turn into hours? Relationships between sustained pupil dilation in response to emotional information and self-reported rumination. Cognitive Therapy Research (2003). 365– 383. \ 6. Steinhauer, S. R., and Hakerem G. The pupillary response in cognitive psychophysiology and schizophrenia. In: Friedman D, Bruder G editors. Psychophysiology and experimental psychopathology: A tribute to Samuel Sutton. Annals of the New York Academy of Sciences (1992), 182–204. \ 7. Pan, J., and Tompkins, W.J. A real-time QRS detection algorithm. IEEE Transactions on biomedical engineering (1985), 230–236. \ 8. Won, M. J., Park, S., Kim, C. J., Lee, E. C., and Whang, M. A Study on Evaluation of Visual Factor for Measuring Subjective Virtual Realization, Journal of the Korea Society for Emotion Sensibility (2012), 373-382. \ ","Virtual Reality; Presence; Human Avatar; Pupillary Response; Heart Rate","I.3.7","int0143-file1.doc","int0143-file2.jpg","","int0143-file4.jpg","int0143-file5.mp4","Development of Realistic Digital Expression of Human Avatars through Pupillary Responses based on Heart Rate.","Pupillary response and Heart rate your virtual avatar, Experience a fun realistic digital expression human avatar with your friends.","","114","Development of Realistic Digital Expression of Human Avatars through Pupillary Responses based on Heart Rate.","Myoung Ju Won","Sangin Park","earlyrelease","","FormatComplete","2015-0029756","Global Frontier R&D Program (MSIP)","","","","","Feb  9 09:54",""
"int144","A","Waving Authentication: Your Smartphone Authenticate You on Motion Gesture","Feng","Hong","hongfeng@ouc.edu.cn","int0144-paper.pdf","4","letter","","","Feng Hong, Meiyu Wei, Shujuan You, Yuan Feng, Zhongwen Guo","hongfeng@ouc.edu.cn, weimeiyu2012@gmail.com, youshujuan@gmail.com, fengyuan@ouc.edu.cn, guozhw@ouc.edu.cn","44887","Feng","","Hong","hongfeng@ouc.edu.cn","Ocean University of China","Computer Science and Technology","Qingdao","Shandong","China","","","","","","44816","Meiyu","","Wei","weimeiyu2012@gmail.com","Ocean University of China","Ocean University of China","Qingdao","Shandong","China","","","","","","46360","Shujuan","","You","youshujuan@gmail.com","Department of Computer Science and Technology ","Ocean University of China","Qingdao","Shandong","China","","","","","","46361","Yuan","","Feng","fengyuan@ouc.edu.cn","Information Science and Engineering College","Ocean University of China","Qingdao","Shandong","China","","","","","","46362","Zhongwen","","Guo","guozhw@ouc.edu.cn","Information Science and Engineering","Ocean University of China","Qingdao","Shandong","China","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","User authentication is important to protect sensitive and private information for smartphone users. \ We propose Waving Authentication (WA) which is a motion gesture authentication system based on accelerometer. WA utilizes eight distinguishing features hiding in the acceleration traces of motion gestures and exploits one-class Support Vector Machine for classification. It is insusceptible to shoulder surfing attacks. In the interactivity, we first \ provide two exhibitors' phones for audiences to try intruding WA by all kinds of waving. And we present our WA app to the audience smartphones, letting the phone to recognize their owners on audiences motion gesture.","Feng Hong","hongfeng@ouc.edu.cn","1. Guo, Y., Yang, L., Ding, X., Han, J., and Liu, Y. Opensesame: Unlocking smart phone through handshaking biometrics. In INFOCOM (2013). \ 2. Liu, J., Zhong, L., Wickramasuriya, J., and Vasudevan, V. User evaluation of lightweight user authentication with a single tri-axis accelerometer. In MobileHCI (2009). \ 3. Matsuo, K., Okumura, F., Hashimoto, M., Sakazawa, S., and Hatori, Y. Arm swing identiﬁcation method with template update for long term stability. In ICB (2007). \ ","behavior-metrics; biometrics; accelerometer; authentication","H.5.2","int0144-file1.zip","int0144-file2.jpg","","int0144-file4.png","int0144-file5.mp4","Waving Authentication: Your Smartphone Authenticate You on Motion Gesture. This smartphone authentication method exploits the unique features inside motion gesture, provides shoulder surf resistance while remaining easy to use.","Waving Authentication exploits the unique features inside motion gesture for smartphone authentication. During the interactivity, you may watch motion gestures and ask how the exhibitor performs, but cannot imitate to unlock. ","","104","Waving Authentication: Your Smartphone Authenticate You on Motion Gesture","Feng Hong","Meiyu Wei","earlyrelease","","FormatComplete","61379128","NSFC","NSFC","61379127","","","Feb 12 22:43",""
"int145","A","Data Transmission Method for Mobile Phone Using Groove Scan Code","JunBong","Song","junbong.song@gmail.com","int0145-paper.pdf","4","letter","Not_a_PDF_file","","Junbong Song, Hyunwoo Bang","junbong.song@gmail.com, savoy@snu.ac.kr","34025","Junbong","","Song","junbong.song@gmail.com","","Seoul National University","Seoul","","Republic of Korea","","Seoul National University","Seoul","","Republic of Korea","28289","Hyunwoo","","Bang","savoy@snu.ac.kr","","Seoul National University","Seoul","","Republic of Korea","","Seoul National University","Seoul","","Republic of Korea","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","A new mobile phone data-transmit method, groove scan code, is suggested for transmitting low-size data. As groove scan code uses audible collision sound generated by scanning on pre-encoded groove pattern as a data source, it can be implemented at minimum cost. By decoding the scanning sound, the original data can be transferred to mobile phone. Through the feasibility tests, we achieved an acceptable data decoding success rate and transmitting speed. The usability and intuitiveness are scored through user interviews.","Junbong Song","Junbong.song@gmail.com","1. http://www.irda.org \ 2. Zimmerman, T.G. ""Personal area networks: nearfield intrabody communication."" IBM systems Journal 35.3.4 1996, p609-617. \ 3. Trifunovic, Sacha, et al. ""Wifi-opp: ad-hoc-less opportunistic networking."" Proceedings of the 6th ACM workshop on Challenged networks. ACM, 2011, p 37-42, \ 4. Kazimierz Siwiak and others, Ultra-Wideband Radio Technology, J Wiley Online Library, 2004 \ 5. ISO/IEC 15420:2000, Bar Code : EAN/UPC. \ 6. ISO/IEC 16022:2000, Data Matrix. \ 7. ISO/IEC 18004:2000, QR Code. \ 8. nttdocomo.com/technologies/future/audio/index.ht ml \ 9. Matsuoka, Hosei, et al. ""Acoustic OFDM: Embedding high bit-rate data in audio."" Advances in Multimedia Modeling. Springer Berlin Heidelberg, 2008. p498-507. \ ","Data transmission; Audio code; Tag; Mobile phone","H.5.2. User Interfaces: input devices and strategies","int0145-file1.doc","int0145-file2.jpg","int0145-file3.mp4","int0145-file4.jpg","int0145-file5.mp4","Groove Scan Code, The new mobile phone data-transmit method","New mobile phone data-transmit method, groove scan code, is suggested for low-size data with low cost. Only with scanning encoded grooved pattern, encoded data transfers to mobile phone!!","","122","Groove Scan Code, The new mobile phone data-transmit method","JunBong Song","Hyunwoo Bang","earlyrelease","","FormatComplete","","","","","","","Feb 16 08:51",""
"int147","A","NOISA: A Novel Intelligent System Facilitating Smart Interaction","Koray","Tahiroğlu","koray.tahiroglu@aalto.fi","int0147-paper.pdf","4","letter","","","Koray Tahiroğlu, Thomas Svedström, Valtteri Wikström","koray.tahiroglu@aalto.fi, thomas.svedstrom@aalto.fi, valtteri.wikstrom@aalto.fi","24847","Koray","","Tahiroğlu","koray.tahiroglu@aalto.fi","Department of Media","Aalto University","Helsinki","","Finland","","","","","","42198","Thomas","","Svedström","thomas.svedstrom@aalto.fi","Department of Media","Aalto University","Helsinki","","Finland","","","","","","34220","Valtteri","","Wikström","valtteri.wikstrom@aalto.fi","Department of Media","Aalto University, School of Arts, Design and Architecture","Helsinki","","Finland","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","In this paper, we describe NOISA (Network of Intelligent Sonic Agents). NOISA is an intelligent system that acts to maintain and deepen the user’s engagement with digital artefacts by learning from the user’s actions and behavioural patterns in the moment of interaction. It facilitates a smart interaction by monitoring user’s bodily movements, facial expressions and control inputs. We present our model and system in a musical context, interfaced with our digital musical instrument (DMI). Our concept can be further extended to possible application areas in Human Computer Interaction (HCI) research field.","Koray Tahiroğlu","koray.tahiroglu@aalto.fi","1. Magnusson, T. Designing constraints: Composing and performing with digital musical systems. Computer Music Journal 34, 4 (2010), 62–73. \ 2. Szaﬁr, D., and Mutlu, B. Pay attention!: designing adaptive agents that monitor and improve user engagement. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, ACM (2012), 11–20. \ 3. Tanaka, A. Sensor-based musical instruments and interactive. The Oxford handbook of computer music (2009), 233. \ 4. von Laban, R., and Ullmann, L. The Mastery of Movement. Dance Books, 2011. \ ","engaging interaction; musical human-computer interaction; NOISA; novel musical instruments;intelligent system; smart interaction","H.5.2 [User Interfaces]: Interaction styles, Input devices and strategies; H.5.5 [Sound and Music Computing]: Systems; I.2.11 [Distributed Artificial Intelligence]: Intelligent agents.","int0147-file1.zip","int0147-file2.jpg","int0147-file3.mp4","int0147-file4.png","int0147-file5.mp4","NOISA is an intelligent system that acts to maintain and deepen the user’s engagement with digital artefacts by learning from the user’s actions and behavioural patterns in the moment of interaction.","Experience a smart and engaging interaction with NOISA musical instruments !","","112","￼NOISA: A Novel Intelligent System Facilitating Smart Interaction","Koray Tahiroğlu","Thomas Svedström","earlyrelease","","FormatComplete","Starting Grant","Aalto University, School of ARTS","","","","","Feb 17 03:12",""
"int148","A","Multi-Player Gaming on Spherical Displays","Julie","Williamson","julie@dcs.gla.ac.uk","int0148-paper.pdf","4","letter","","","Julie R Williamson, John Williamson, Daniel Sundén, Jay Bradley","Julie.Williamson@glasgow.ac.uk, johnh.williamson@glasgow.ac.uk, daniel@sunden.se, jay@pufferfishdisplays.co.uk","13853","Julie","R","Williamson","Julie.Williamson@glasgow.ac.uk","School of Computing Science","University of Glasgow","Glasgow","","United Kingdom","","","","","","40932","John","","Williamson","johnh.williamson@glasgow.ac.uk","School of Computing Science","University of Glasgow","Glasgow","","United Kingdom","","","","","","46457","Daniel","","Sundén","daniel@sunden.se","School of Computing Science","University of Glasgow","Glasgow","Lanarkshire","United Kingdom","","","","","","46458","Jay","","Bradley","jay@pufferfishdisplays.co.uk","","Pufferfish","Edinburgh","","United Kingdom","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Spherical displays offer unique affordances for multi-player games and playful interactions in social spaces.  The shape of a spherical display allows users to face each other and maintain eye contact during interaction, creating a different social dynamic than at a flat display.  There is also no intrinsically defined front or centre of the display, offering different views from different viewing angles. This creates shared and private areas of the display given users’ varying perspectives.  Trajectory based games have a dramatically different experience when played on a spherical surface.  Side-scrolling games are also exciting on a spherical surface, becoming “rotating” games where users’ action affect others playing at different points around the screen.  This Interactivity exhibit showcases two multi-player games that specifically exploit the affordances of a spherical display in a social setting.  ","Julie Williamson","Julie.WIlliamson@glasogw.ac.uk","[1]	Benko, H., Wilson, A.D., and Balakrishnan, R. Sphere: multi-touch interactions on a spherical display. In Proc. of UIST '08. ACM, New York, NY, USA, 77-86.  \ [2]	Bolton, J., Kim, K., and Vertegaal, R.  A comparison of competitive and cooperative task performance using spherical and flat displays. In Proc. of CSCW '12. ACM, New York, NY, USA, 529-538.  \ [3]	Machida T.  GEO-COSMOS: world's first spherical display. In ACM SIGGRAPH 2002. ACM, New York, NY, USA, 189-189.   \ ","Spherical displays, multi-player games, playful interfaces.","H.5.0 Information Interfaces and Presentation (HCI): General.","int0148-file1.docx","int0148-file2.jpg","int0148-file3.mp4","int0148-file4.jpg","","A spherical volley game takes a playful twist on an arcade classes.","Spherical displays offer unique affordances for multi-player games and playful interactions.  This Interactivity exhibit showcases two multi-player games that specifically exploit the affordances of a spherical display in a social setting.","","201","Multi-Player Gaming on Spherical Displays","Daniel Sundén","Julie Williamson","earlyrelease","","FormatComplete","EP/M002675/1","EPSRC","","","","","Feb  2 14:25",""
"int149","A","Sustainable Transport System: A Wheel Based Interactive Information Installation","GEON DONG","KIM","geon705@gmail.com","int0149-paper.pdf","4","letter","","","Geon Dong Kim, Juhyun Eune","geon705@gmail.com, jheune@snu.ac.kr","47770","Geon Dong","","Kim","geon705@gmail.com","Digital Media Design","Hongik University","Sejong","","Korea","","","","","","30944","Juhyun","","Eune","jheune@snu.ac.kr","Intermedia Lab","Seoul National University","Seoul","","Republic of Korea","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Sustainable Transport System is an interactive wheel based information installation where users can watch the information projected on the wheel with a narration. Its information consists of 9 questions that are related with road transport systems ranging from 'The history of road traffic' to 'How will the sustainable traffic system evolve in the future?’ The circular interface was used to show information in a pie chart, diagram and history of wheel. This interface contains the meaning of sustainable circulation. The modalities of the project are Vision, Sonic, and Touch. A potentiometer sensor is mounted onto the center of wheel, which is linked with Flash action script through Arduino as the technical method. The user enables the information to be navigated by rotating the wheel clockwise from No.1 to No.9 and counterclockwise from No.9 to No.1. CHI attendees can experience the information ranging from history of road transport to necessity of sustainable transport system easily, interestingly and engagingly.","Geon Dong Kim","geon705@gmail.com","1. Juno, La, The future of urban transportation, huge mobility ecosystem. LG Business Insight March 2014. \ 2. Korean Statistical Information Service http://kosis.kr/ \ 3. Nathan Shedroff, Information Interaction Design: A Unified Field Theory of Design http://www.nathan.com/thoughts/unified/ \ 4. Arduino http://arduino.cc/en/main/software \ 5. Donald, A. N., The Design of Everyday Things. Basic Books; Revised Edition, November 2013, 82-88. \ ","Physical wheel interface; Interactive information; Rotating interaction; Projection mapping","H.5.m","int0149-file1.doc","int0149-file2.jpg","int0149-file3.mp4","int0149-file4.jpg","int0149-file5.mp4","Sustainable Transport System: A wheel based interactive information installation","CHI attendees can experience the information ranging from history of road transport to necessity of sustainable transport system easily, interestingly and engagingly.","","206","Sustainable Transport System: A Wheel Based Interactive Information Installation","Geon Dong Kim","Juhyun Eune","earlyrelease","","FormatComplete","","","","","","","Feb 12 19:06",""
"int150","A","Smart Eyewear for Interaction and Activity Recognition","Kai","Kunze","kai.kunze@gmail.com","int0150-paper.pdf","4","letter","","","Shoya Ishimaru, Kai Kunze, Katsuma Tanaka, Yuji Uema, Koichi Kise, Masahiko Inami","ishimaru@m.cs.osakafu-u.ac.jp, kai.kunze@gmail.com, katsuma@m.cs.osakafu-u.ac.jp, uema@kmd.keio.ac.jp, kise@cs.osakafu-u.ac.jp, inami@inami.info","33931","Shoya","","Ishimaru","ishimaru@m.cs.osakafu-u.ac.jp","","Osaka Prefecture University","Osaka","","Japan","","","","","","6054","Kai","","Kunze","kai.kunze@gmail.com","Graduate School of Media Design","Keio University","Yokohama City","Kanagawa","Japan","","","","","","34014","Katsuma","","Tanaka","katsuma@m.cs.osakafu-u.ac.jp","","Osaka Prefecture University","Osaka","","Japan","","","","","","27210","Yuji","","Uema","uema@kmd.keio.ac.jp","","Keio University","Yokohama City","Kanagawa","Japan","","","","","","33932","Koichi","","Kise","kise@cs.osakafu-u.ac.jp","","Osaka Prefecture University","Osaka","","Japan","","","","","","10126","Masahiko","","Inami","inami@inami.info","","Keio University","Yokohama City","Kanagawa","Japan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","vice class with a lot of possibilities for user interac- tion design and unobtrusive activity tracking. In this paper we show applications using an early prototype of J!NS MEME, smart glasses with integrated electrodes to detect eye movements (Electrooculography, EOG) and motion sensors (accelerometer and gyroscope) to monitor head motions. We present several demonstrations: We show a simple eye movement visualization, detecting left/right eye motion and blink. Additionally, users can play a game, ’Blinky Bird’. They need to help a bird avoid obstacles using eye movements. We implemented online detection of reading and talking behavior using a combination of blink, eye movement and head motion. We can give people a long term view of their reading, talking, and also walking activity over the day.","Kai Kunze","kai.kunze@gmail.com","1. Bulling, A., Ward, J. A., Gellersen, H., and Tr¨oster, G. Eye Movement Analysis for Activity Recognition Using Electrooculography. IEEE Trans. PAMI 33, 4 (Apr. 2011), 741–753. \ 2. Ishimaru, S., Kunze, K., Kise, K., Weppner, J., Dengel, A., Lukowicz, P., and Bulling, A. In the blink of an eye: combining head motion and eye blink frequency for activity recognition with google glass. In Augmented Human, ACM (2014), 15. \ 3. Just, M. A., and Carpenter, P. A. The psychology of reading and language comprehension. Allyn & Bacon, 1987. \ 4. Kunze, K., Iwamura, M., Kise, K., Uchida, S., and Omachi, S. Activity recognition for the mind: Toward a cognitive quantiﬁed self. Computer 46, 10 (2013), 0105–108. \ 5. Kunze, K., Katsutoshi, M., Uema, Y., and Inami, M. How much do you read? – counting the number of words a user reads using electrooculography. In Accepted at Augmented Human’15, ACM (2015), 1073– 1078. \ 6. Landman-Peeters, K., Hartman, C. A., van der Pompe, G., den Boer, J. A., Minderaa, R. B., and Ormel, J. Gender diﬀerences in the relation between social support, problems in depression and anxiety. Social Science & Medicine 60, 11 (2005), 2549–2559. \ 7. Manabe, H., and Fukumoto, M. Full-time wearable headphone-type gaze detector. In CHI’06 Extended Abstracts, ACM (2006), 1073–1078. \ 8. Ross, C. E., and Mirowsky, J. Explaining the social patterns of depression: Control and problem solving– or support and talking? Journal of health and social behavior (1989), 206–219. \ 9. Stanovich, K., and Cunningham, A. What reading does for the mind. Journal of Direct Instructions 1, 2 (2001), 137–149. \ ","Eye Movement Analysis; Electrooculography; Eye wear; Smart Glasses; Eye Wear Computing; Activity Recognition","I.5.4","int0150-file1.zip","int0150-file2.jpg","int0150-file3.mp4","int0150-file4.jpg","","Smart Eye Wear is a fairly novel device class with a lot of potential. We present applications using an early prototype of J!NS MEME, smart glasses that can detect eye movements and head motions. ","Smart Eye Wear is a novel device class with potential. We present applications using an early prototype of J!NS MEME, smart glasses that can detect head and eye movements. ","","119","Smart Eye Wear","Kai Kunze","Shoya Ishimaru","earlyrelease","","FormatComplete","","","","","","","Feb  2 06:29",""
"int151","A","Datawear: Self-reflection on the Go or How to Ethically Use Wearable Cameras for Research","Anya","Skatova","anya.skatova@gmail.com","int0151-paper.pdf","4","letter","Helvetica,Bold","","Anya Skatova, Victoria E Shipp, Lee Spacagna, Benjamin Bedwell, Ahmad Beltagui, Tom Rodden","anya.skatova@gmail.com, victoria.shipp@nottingham.ac.uk, lee.spacagna@nottingham.ac.uk, benjamin.bedwell@nottingham.ac.uk, a.beltagui@wlv.ac.uk, tar@Cs.Nott.AC.UK","47783","Anya","","Skatova","anya.skatova@gmail.com","Horizon Digital Economy Research","University of Nottingham ","Nottingham","","UK","","","","","","14944","Victoria","E","Shipp","victoria.shipp@nottingham.ac.uk","Horizon Digital Economy Research","University of Nottingham","Nottingham","","United Kingdom","","","","","","47814","Lee","","Spacagna","lee.spacagna@nottingham.ac.uk","Horizon Digital Economy Research","University of Nottingham","Nottingham","","United Kingdom","","","","","","12554","Benjamin","","Bedwell","benjamin.bedwell@nottingham.ac.uk","Horizon Digital Economy Research","University of Nottingham","Nottingham","","United Kingdom","","","","","","47815","Ahmad","","Beltagui","a.beltagui@wlv.ac.uk","Business School","University of Wolverhampton","Wolverhampton","","United Kingdom","","","","","","1132","Tom","","Rodden","tar@Cs.Nott.AC.UK","School of Computer Science","The University of Nottingham","Nottingham","Nottinghamshire","United Kingdom","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","A growing number of studies use wearable sensors, including cameras, to detect user activity patterns. When an object of academic investigation, these patterns are interpreted by researchers and conclusions are drawn about people’s habits and routines. Alternatively, interpretations are provided by users themselves during extensive post-study interviews. Such approaches inevitably expose personal data collected about individuals to researchers, which can potentially change the behavior under investigation. We introduce a new approach to using wearable sensor data in research. It allows people to interpret and self-reflect on their data and submit for investigation only reflections, without sharing their raw data. In this interactivity, we present and discuss the Datawear mobile application prototype, which is designed to conduct “in the wild” studies of personal experiences. ","Anya Skatova","anya.skatova@gmail.com","1. Gorber, S. C., Schofield-Hurwitz, S., Hardt, J., Levasseur, G. and Tremblay, M. The accuracy of selfreported smoking: a systematic review of the relationship between self-reported and cotinineassessed smoking status. Nicotine & Tobacco Research, 11, 1 2009), 12-24. \ 2. Yürüten, O., Zhang, J. and Pu, P. H. Predictors of life satisfaction based on daily activities from mobile sensor data. In Proc. CHI 2014, ACM Press (2014), 497-500. \ 3. Doherty, A., Williamson, W., Hillsdon, M., Hodges, S., Foster, C. and Kelly, P. Influencing health-related behaviour with wearable cameras: strategies & ethical considerations. In Proc. SenseCam & Pervasive Imaging 2013, ACM Press (2013), 60-67. \ 4. Sellen, A. J., Fogg, A., Aitken, M., Hodges, S., Rother, C. and Wood, K. Do life-logging technologies support memory for the past?: an experimental study using sensecam. In Proc. CHI 2007, ACM Press (2007), 81-90. \ 5. Isaacs, E., Konrad, A., Walendowski, A., Lennig, T., Hollis, V. and Whittaker, S. Echoes from the past: how technology mediated reflection improves well-being. In Proc. CHI 2013, ACM Press (2013), 1071-1080. \ 6. Shipp, V., Skatova, A., Blum, J. and Brown, M. The ethics of wearable cameras in the wild. In Proc. Ethics in Science, Technology and Engineering 2014, IEEE (2014), 1-5. \ 7. McMillan, D., Morrison, A. and Chalmers, M. Categorised ethical guidelines for large scale mobile HCI. In Proc. CHI 2013, ACM Press (2013), 1853-1862. \ 8. Doherty, A. R. and Smeaton, A. F. Automatically segmenting lifelog data into events. In WIAMIS’08, IEEE, 20-23. \ ","Experience Sampling; Self-Reflection; Wearable Cameras; Sensors; Ethics","H.5.2.","int0151-file1.doc","","int0151-file3.mp4","int0151-file4.jpg","","","Experience a new form of data collection with a Datawear app. You can collect the images of your everyday experiences and provide reflections about them on the go.   ","","123","Datawear","Victoria Shipp","Ben Bedwell","earlyrelease","","FormatComplete","EP/G065802/1","EPSRC UK","","","","","Feb  6 15:36",""
"int152","A","TESSA - Toolkit for Experimentation  with Multimodal Sensory Substitution and Augmentation","Carlos","Sainz Martinez","c.sainzmartinez@pgr.reading.ac.uk","int0152-paper.pdf","4","letter","","","Carlos Sainz Martinez, Faustina Hwang","c.sainzmartinez@pgr.reading.ac.uk, f.hwang@reading.ac.uk","47825","Carlos","","Sainz Martinez","c.sainzmartinez@pgr.reading.ac.uk","School of Systems Engineering","University of Reading","Reading","Berkshire","United Kingdom","","","","","","1571","Faustina","","Hwang","f.hwang@reading.ac.uk","School of Systems Engineering","The University of Reading","Reading","Berkshire","United Kingdom","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","TESSA is a toolkit for experimenting with sensory augmentation. It includes hardware and software to facilitate rapid prototyping of interfaces that can enhance one sense using information gathered from another sense. The toolkit contains a range of sensors (e.g. ultrasonics, temperature sensors) and actuators (e.g. tactors or stereo sound), designed modularly so that inputs and outputs can be easily swapped in and out and customized using TESSA’s graphical user interface (GUI), with “real time” feedback.  The system runs on a Raspberry Pi with a built-in touchscreen, providing a compact and portable form that is amenable for field trials.  At CHI Interactivity, the audience will have the opportunity to experience sensory augmentation effects using this system, and design their own sensory augmentation interfaces.","Carlos Sainz Martinez","c.sainzmartinez@pgr.reading.ac.uk","1. Auvray, M., Hanneton, S. and O’Regan, K. J. Learning to perceive with a visuo-auditory substitution system: Localisation and object recognition with “The vOICe”. Perception 36 (2007) 416-430 \ 2. Bach-y-Rita, P., Collins, C.C., Saunders, F.A., White, B. and Scadden, L. Vision Substitution by Tactile Image Projection. Nature 221 (1969) 963-964 \ 3. Cao, Y., Mattisson, S. and Bjork, C. SeeHear System: A New Implementation Solid-State Circuits Conference (1992) 199-202 \ 4. Carton, A. and Dunne, L.E. Tactile Distance Feedback for Firefighters: Design and Preliminary Evaluation of a Sensory Augmentation Glove. 4th Augmented Human International Conference (2013) 58-63 \ 5. Nagel, S.K., Carl, C., Kringe, T., Märtin, R. and König, P.M. Beyond sensory substitution – learning the sixth sense. Journal of Neural Engineering 2(4) (2005) 13-26 \ 6. Roentgen, U.R., Gelderblom, G.J., Soede, M., and de Witte, L.P. The Impact of Electronic Mobility Devices for Persons Who Are Visually Impaired: A Systematic Review of Effects and Effectiveness. Journal of Visual Impairment & Blindness 103(11) (2009) 743-753 \ 7. Rosenthal, J., Edwards, N., Villanueva, D., Krishna, S., McDaniel, T. and Panchanathan, S. Design, Implementation and Case Study of a Pragmatic Vibrotactile Belt. IEEE Transactions on Instrumentation and Measurement 1(60) (2011) 114-125","Sensory Augmentation; Sensory Substitution; Haptics, Audio; Toolkit; Modular Design","H.5.2","int0152-file1.docx","int0152-file2.jpg","","int0152-file4.jpg","int0152-file5.mp4","Logo for the Toolkit for Experimentation with Sensory Substitution and Augmentation.","Experience Sensory Augmentation! With TESSA you can hear North, feel temperatures at a distance, or sense walls while blindfolded - and many more combinations that you can design and assemble yourself!","","103","Toolkit for Experimentation with Sensory Substitution and Augmentation","Carlos Sainz Martinez","Faustina Hwang","earlyrelease","","FormatComplete","","","","","","","Feb 16 10:28",""
"int154","A","WoBo: Multisensorial travels through Oculus Rift","Lucio Davide","Spano","davide.spano@unica.it","int0154-paper.pdf","4","letter","","","Stefano Fibbi, Fabio Sorrentino, Lucio Davide Spano, Riccardo Scateni","stefano.fibbi@gmail.com, sorrefabio@gmail.com, davide.spano@unica.it, riccardo@unica.it","47829","Stefano","","Fibbi","stefano.fibbi@gmail.com","Mathematics and Computer Science","University of Cagliari","Cagliari","","Italy","","","","","","42025","Fabio","","Sorrentino","sorrefabio@gmail.com","Mathematics and Computer Science","University of Cagliari","Cagliari","","Italy","","","","","","47522","Lucio Davide","","Spano","davide.spano@unica.it","Mathematics and computer science","University of Cagliari","Cagliari","","Italy","","","","","","16217","Riccardo","","Scateni","riccardo@unica.it","Mathematics and Computer Science","University of Cagliari","Cagliari","","Italy","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","WoBo (World in a Box) aims to provide a new experience for travellers, allowing them to visit distant or hardly reachable places through the exploitation of consumer cameras and a head mounted display. The experience consists in watching a 360-degrees video with 3D audio in a dedicated cabin. The user can select videos shot in different places, which have been created with six consumer cameras. We describe the proposed experience, the hardware and the software used for a first prototype.","Fabio Sorrentino","sorrefabio@gmail.com","1. Lowe, D. G. Object recognition from local scale-invariant features. In Computer vision, 1999. The proceedings of the seventh IEEE international conference on, vol. 2, Ieee (1999), 1150–1157. \ 2. Witmer, B., and Singer, M. Measuring presence in virtual environments: A presence questionnaire. Presence 3, 7 (1998), 225–240. 2http://www.samsung.com/global/microsite/gearvr/ \ ","Presence; Multisensorial; Head Mounted Displays","H.5.m","int0154-file1.zip","int0154-file2.jpg","int0154-file3.mp4","int0154-file4.png","int0154-file5.mp4","World in a Box, just like being there","Why looking for a genius to get it out of the box? Enter in WoBoX and go wherever you want, even without a genius!","","117","WoBo: Multisensorial travels through Oculus Rift","Fabio Sorrentino","Lucio Davide Spano","earlyrelease","","FormatComplete","POR FSE Axis IV, Obj l.3, LoA 1.3.1","Sardinia Regional Government","","","","","Feb 16 06:16",""
"int155","A","Canvas Dance: An Interactive Dance Visualization for Large-Group Interaction","Carla","Griggio","carla.griggio@gmail.com","int0155-paper.pdf","4","letter","","","Carla F Griggio, Mario Romero","carla.griggio@gmail.com, marior@kth.se","41183","Carla","F","Griggio","carla.griggio@gmail.com","CSC","KTH Royal Institute of Technology","Stockholm","","Sweden","Master School","EIT ICT Labs","Stockholm","Stockholm","Sweden","5888","Mario","","Romero","marior@kth.se","CSC","KTH","Stockholm","","Sweden","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","We present Canvas Dance, a prototype of an interactive dance visualization for large-group interaction that targets non-professional dancers in informal environments such as parties or nightclubs, and uses the smartphones of the dancers as the input device for the motion signal. The visualization is composed of individual representations for each dancer, and the visual mappings designed for their dance moves have three main goals: to help the users identify their own representation, to uncover and inspire imitation among dancers, and to support unpredictable dance moves. ","Carla F. Griggio","carla.griggio@gmail.com","1. B. Gonzalez, E. Carroll, and C. Latulipe, “Danceinspired technology, technology-inspired dance,” in Proceedings of the 7th Nordic Conference on Human-Computer Interaction: Making Sense Through Design, 2012, pp. 398–407. \ 2. A. Bayliss, J. G. Sheridan, and N. Villar, “New shapes on the dance floor: influencing ambient sound and vision with computationally augmented poi,” Int. J. Perform. Arts Digit. Media, vol. 1, no. 1, pp. 67–82, 2005. \ 3. M. Feldmeier and J. A. Paradiso, “An interactive music environment for large groups with giveaway wireless motion sensors,” Comput. Music J., vol. 31, no. 1, pp. 50–67, 2007. \ 4. J. Randall, “Real-time lighting system for large group interaction,” SB Thesis Mass. Inst. Technol. Camb. Mass., 2002. \ 5. C. Griggio, G. Leiva, and V. V. Juarranz, “Canvas Dance: an interactive dance visualization prototype for nightclubs,” presented at the SIDER ’14, 2014. \ 6. R. Hodgin, “Magnetosphere iTunes Visualizer.” http://roberthodgin.com/magnetosphere-itunesvisualizer/. \ 7. J. Bertin, Sémiologie graphique: Les diagrammes Les réseaux - Les cartes. Editions de l’Ecole des Hautes Etudes en Sciences, 1999. \ ","Interactive dance visualizations; large-group interaction","H.5.m","int0155-file1.doc","","int0155-file3.mp4","int0155-file4.jpg","int0155-file5.mp4","","Download the Canvas Dance app to your phone and save it in a pocket. Dance with others to animate the visuals on the screen and create cool visual effects together!","","207","Canvas Dance","Carla Griggio","Mario Romero","earlyrelease","","FormatComplete","","","","","","","Feb 16 19:13",""
"int158","A","Nebula: An Interactive Garment Designed for Functional Aesthetics","Ludvig","Elblaus","elblaus@kth.se","int0158-paper.pdf","4","letter","","","Ludvig Elblaus, Vasiliki Tsaknaki, Vincent Lewandowski, Roberto Bresin","elblaus@kth.se, tsaknaki@kth.se, vlew@kth.se, roberto@kth.se","34289","Ludvig","","Elblaus","elblaus@kth.se","MID","KTH Royal Institute of Technology","Stockholm","","Sweden","","","","","","34283","Vasiliki","","Tsaknaki","tsaknaki@kth.se","Mobile Life Centre","VINN EXCELLENCE CENTRE","KISTA ","STOCKHOLM","Sweden","","","","","","38461","Vincent","","Lewandowski","vlew@kth.se","MID","KTH Royal Institute of Technology","Stockholm","","Sweden","","","","","","47882","Roberto","","Bresin","roberto@kth.se","MID","KTH Royal Institute of Technology","Stockholm","","Sweden","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","In this paper we present Nebula, a prototype for examining the properties of textiles, fashion accessories, and digital technologies to arrive at a garment design that brings these elements together in a cohesive manner. Bridging the gap between everyday performativity and enactment, we aim at discussing aspects of the making process, interaction and functional aesthetics that emerged. \  \ Nebula is part of the Sound Clothes project that aims at exploring the expressive potential of wearable technologies creating sound from motion. \ ","Ludvig Elblaus","elblaus@kth.se","1. Dunne, A., Raby, F. Speculative Everything: Design, Fiction, and Social Dreaming. MIT Press. London, (2013), 88. \ 2. Ryan, Susan E. Re-Visioning the Interface: Technological Fashion as Critical Media. Leonardo Vol. 42, (2009), 307-313. \ 3. Schencher, R. Performance Studies: An Introduction. Routledge. Cornwall, (2006). \ 4. Madgwick, S., Mitchell T. x-OSC: A Versatile Wireless I/O Device For Creative/Music Applications, Proc. of the Sound and Music Computing Conference 2013, Stockholm, (2013), 213–219. Figure 5: The design of the garment evokes images of star fields and nebulas. \ ","Wearable technology; design process; fashion; music computation","H.5.m; H.5.5:","int0158-file1.doc","","","int0158-file4.jpg","int0158-file5.mp4","","Galactic sounds from a myriad of studs: The Nebula interactive garment reacts to the subtle movements of the wearer by creating expressive soundscapes, blurring the boundaries between fashion and performativity.","","111","Nebula: An Interactive Garment Designed for Functional Aesthetics","Ludvig Elblaus","VASILIKI TSAKNAKI","earlyrelease","","FormatComplete","","","","","","","Feb 16 11:26",""
"int159","A","Digiti Sonus v2: New Interface for Fingerprint Data Sonification using Hand Motion","Yoon Chung","Han","hanyoonjung@gmail.com","int0159-paper.pdf","4","letter","","","Yoon Chung Han, Byeong-jun Han","yoon@mat.ucsb.edu, hbj1147@korea.ac.kr","47846","Yoon Chung","","Han","yoon@mat.ucsb.edu","Media Arts and Technology","University of California, Santa Barbara","Santa Barbara","California","United States","","","","","","47902","Byeong-jun","","Han","hbj1147@korea.ac.kr","School of Electrical Engineering","Korea University","Seoul","","South Korea","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Digiti Sonus v2 is a new interface for fingerprint data sonification using audience’s biometric data and apply their hand motions to control and modify their own audiovisual contents. This interface explores users’ fingerprint data as personalized artistic materials, and allows them to rearrange and compare the data with others as pieces of a sonic puzzle by hand motion. This work expands the possibility of creating diverse audiovisual results based on users’ interaction, and enhances easier and more intuitive interaction with hand motion using short-range depth camera.","Yoon Chung Han","yoon@mat.ucsb.edu","1. Han, Y., Han, B., and Wright, M., ""Digiti Sonus: Advanced Interactive Fingerprint Sonification using Visual Feature Analysis,"" in Proceedings of the International Conference on New Interfaces for Musical Expression (NIME) (2013). \ 2. Han, Y., and Han, B., ""Digiti Sonus,"" Leonardo Special Issue: SIGGRAPH 2013 Art Papers and XYZN: ScaleArt Gallery, Leonardo (2013), vol.46, no.4, 392393. \ 3. Han, Y., and Han, B., ""Skin Pattern Sonification Using NMF-based Visual Feature Extraction and Learning-based PMSon,"" in Proceedings of the International Conference on Auditory Display (ICAD), New York University (2014). \ 4. Hermann, T., and Ritter, H., ""Sound and Meaning in Auditory Data Display,"" in Proceedings of IEEE (2004), vol.92, no.4, 730-741. \ 5. Geco: Multi-dimensional midi expression through hand gestures. http://www.uwyn.com/geco . \ 6. Wanderley, M.M., ""Gestural control of music,"" in Proceedings of International Workshop Human Supervision and Control in Engineering and Music (2001), 632-644. \ 7. Leap Motion. https://www.leapmotion.com . Figure 4. Implementation of Digiti Sonus v2. 2D Array layout (first), a zoomed fingerprint image from array layout (second), random layout (third), and a zoomed fingerprint image from random layout (fourth) \ ","fingerprint; fingerprint sonification; interface design; hand motion; biometric; personalized art; leap motion; interactive art; data sonification","H.5.m; H.5.2; ","int0159-file1.doc","int0159-file2.jpg","","int0159-file4.jpg","int0159-file5.mp4","A zoomed fingerprint image on a 2D array layout in Digiti Sonus v2.","Digiti Sonus v2 is a new interface for fingerprint data sonification using audience’s biometric data and apply their hand motions to control and modify their own audiovisual contents. ","","203","Digiti Sonus v2: New Interface for Fingerprint Data Sonification using Hand Motion ","Yoon Chung Han","Byeong-jun Han","earlyrelease","","FormatComplete","","","","","","","Feb 15 23:44",""
"int164","A","The Art.CHI Gallery: An Embodied Iterative Curation Experience","Nic","Lupfer","nic@ecologylab.net","int0164-paper.pdf","4","letter","","","Nic Lupfer, Bill Hamilton, Andrew Webb, Rhema Linder, Ernest Edmonds, Andruid Kerne","nic@ecologylab.net, bill@ecologylab.net, andrew@ecologylab.net, rhema@ecologylab.net, ernest@ernestedmonds.com, andruid@ecologylab.net","28529","Nic","","Lupfer","nic@ecologylab.net","Interface Ecology Lab","Texas A&M University","College Station","Texas","United States","","","","","","28640","Bill","","Hamilton","bill@ecologylab.net","","Interface Ecology Lab @ Texas A&M University","College Station","TX","United States","","","","","","8935","Andrew","","Webb","andrew@ecologylab.net","Interface Ecology Lab","Texas A&M University","College Station","Texas","United States","","","","","","30053","Rhema","","Linder","rhema@ecologylab.net","Interface Ecology Lab","Texas A&M University","College Station","Texas","United States","","","","","","1938","Ernest","","Edmonds","ernest@ernestedmonds.com","","University of Technology, Sydney","Sydney","NSW","Australia","","","","","","3309","Andruid","","Kerne","andruid@ecologylab.net","Interface Ecology Lab","Texas A&M University","College Station","Texas","United States","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","We present an exhibition of the 2015 Art.CHI Gallery as an embodied and iterative curation experience. We develop a method to represent the initial curated collection of interactive art works as a holistic exhibition. We explore how to physically manifest and present the exhibition at the CHI conference. Our exhibition will invite visitors to interact with the gallery as an embodied experience. They will browse the exhibit, as a whole, and view details of individual works. Participants will be encouraged to iteratively curate their own miniature gallery as they peruse. These curations will be made accessible online and shared through social media.","Nic Lupfer","nic@ecologylab.net","1. Cook, S. Immateriality and its discontents: models of curating new media art. In New Media in the White Cube and Beyond, C. Paul, Ed. University of California Press, 2008. \ 2. Durrant, A., Rowland, D., Kirk, D. S., Benford, S., Fischer, J. E., and McAuley, D. Automics: Souvenir generating photoware for theme parks. In Proc CHI (2011), 1767–1776. \ 3. Edmonds, E., Benford, S., Bilda, Z., Fantauzzacoﬃn, J., Malina, R., and Vinet, H. Digital arts: Did you feel that? In CHI Extended Abstracts (2013), 2439–2446. \ 4. England, D., Candy, L., Latulipe, C., Schiphorst, T., Edmonds, E., Kim, Y., Clark, S., and Kerne, A. Art.chi: The workshop. In CHI Extended Abstracts (2015). \ 5. England, D., Spence, J., Latulipe, C., Edmonds, E., Candy, L., Schiphorst, T., Bryan-Kinns, N., and Woolford, K. Curating the digital: Spaces for art and interaction. In CHI Extended Abstracts (2014). \ 6. Kerne, A., Webb, A. M., Smith, S. M., Linder, R., Lupfer, N., Qu, Y., Moeller, J., and Damaraju, S. Using metrics of curation to evaluate information-based ideation. ACM ToCHI 21, 3 (2014), 1–48. \ 7. Lupfer, N., Kerne, A., Linder, R., Qu, Y., and Webb, A. M. Beyond the feed and board: Holistic principles for expressive web curation. ACM To CHI (In submission). \ 8. Webb, A., Kerne, A., Linder, R., Lupfer, N., Qu, Y., Keith, K., and Carrasco, M. Multi-scale information composition: a new medium for freeform art curation in the cloud. In CHI Workshop: Curating the Digital: Spaces for Art and Interaction (2014). \ ","Art Exhibition; Digital Art; Performance Art; Interaction;  Interdisciplinary; Curation; Installation","H.5.m","int0164-file1.zip","int0164-file2.jpg","","int0164-file4.png","","The Art.CHI Gallery: An Embodied Iterative Curation Experience. \ A diagram of the experience showing one participant moves her body to explore the gallery as others design their own curations.","Experience the 2015 Art.CHI Gallery! Use your body to explore the digital exhibition. Design and share your own curations.","","210","The Art.CHI Gallery: An Embodied Iterative Curation Experience","Nic Lupfer","William A. Hamilton","earlyrelease","","FormatComplete","1247126","National Science Foundation","","","","","Feb  9 10:22",""
"int165","A","Filteryedping: A Dwell-Free Eye Typing Technique","Diogo","Pedrosa","diogo@icmc.usp.br","int0165-paper.pdf","4","letter","","","Diogo Pedrosa, Maria da Graça Pimentel, Khai N Truong","diogo@icmc.usp.br, mgp@icmc.usp.br, ktruong8@uncc.edu","24567","Diogo","","Pedrosa","diogo@icmc.usp.br","ICMC","University of Sao Paulo","Sao Carlos","Sao Paulo","Brazil","University of North Carolina at Charlotte, Charlotte, North Carolina, United States","","","","","39358","Maria da Graça","","Pimentel","mgp@icmc.usp.br","ICMC","University of Sao Paulo","Sao Carlos","Sao Paulo","Brazil","","","","","","2470","Khai","N","Truong","ktruong8@uncc.edu","","University of North Carolina at Charlotte","Charlotte","North Carolina","United States","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","The ability to type using eye gaze only is extremely important for individuals with a severe motor disability. To eye type, the user currently must sequentially gaze at letters in a virtual keyboard and dwell on each desired letter for a specific amount of time to input that key. Dwell-based eye typing has two possible drawbacks: unwanted input if the dwell threshold is too short or slow typing rates if the threshold is long. We demonstrate an eye typing technique, which does not require the user to dwell on the letters that she wants to input. Our method automatically filters out unwanted letters from the sequence of letters gazed at while typing a word. It ranks candidate words based on their length and frequency and presents them to the user for confirmation. Spell correction and support for typing words not in the corpus are also included.","Diogo Pedrosa","diogocpedrosa@gmail.com","1. Bee, N. and André, E. Writing with Your Eye: A Dwell Time Free Writing System Adapted to the Nature of Human Eye Gaze. In Proc. PIT 2008, Springer (2008), 111–122. \ 2. Chakraborty, T., Sarcar, S., and Samanta, D. Design and evaluation of a dwell-free eye typing technique. In Ext. Abstracts CHI 2014, ACM Press (2014), 1573–1578. \ 3. Davies, M. The corpus of contemporary American English (COCA): 450 million words, 1990-2012. http://www.americancorpus.org (2008). \ 4. Hoppe, S., Löchtefeld, M., and Daiber, F. Eype – Using Eye-Traces for Eye-Typing. In Workshop on Grand Challenges in Text Entry (CHI 2013). \ 5. Isokoski, P. Text input methods for eye trackers using off-screen targets. In Proc. ETRA 2000, ACM Press (2000), 15–21. \ 6. Kristensson, P.O. and Vertanen, K. The potential of dwell-free eye-typing for fast assistive gaze communication. In Proc. ETRA 2012, ACM Press (2012) 241–244. \ 7. MacKenzie, I.S. and Soukoreff, R.W. Phrase sets for evaluating text entry techniques. In Ext. Abstracts CHI 2003, ACM Press (2003), 754–755. \ 8. Majaranta, P., Ahola, U.K., and Špakov, O. Fast gaze typing with an adjustable dwell time. In Proc. CHI 2009, ACM Press (2009), 357–360. \ 9. Morimoto, C.H. and Amir, A. Context switching for fast key selection in text entry applications. In Proc. ETRA 2010, ACM Press (2010), 271–274. \ 10. Pedrosa, D., Pimentel, M.G., Wright, A., and Truong, K.N. 2015. Filteryedping: Design challenges and user performance of dwell-free eye typing. TO APPEAR in TACCESS. 40 pages. \ 11. Räihä, K.J. and Ovaska, S. An exploratory study of eye typing fundamentals: dwell time, text entry rate, errors, and workload. In Proc. CHI 2012, ACM Press (2012), 3001–3010. \ 12. Rough, D., Vertanen, K., and Kristensson, P.O. An evaluation of Dasher with a high-performance language model as a gaze communication method. In Proc. AVI 2014, ACM Press (2014), 169–176. \ 13. Ward, D.J., Blackwell, A.F., and MacKay, D.J.C. Dasher – a data entry interface using continuous gestures and language models. In Proc. UIST 2000, ACM Press (2000), 129–137. \ 14. Wobbrock, J.O., Rubinstein, J., Sawyer, M.W., and Duchowski, A.T. Longitudinal evaluation of discrete consecutive gaze gestures for text entry. In Proc. ETRA 2008, ACM Press (2008), 11–18. \ ","Gaze; Text Entry; Eye Typing; Dwell-Free; Motor Disability","H.5.2","int0165-file1.doc","int0165-file2.jpg","int0165-file3.mp4","int0165-file4.png","int0165-file5.mp4","A user of Filteryedping in front of a monitor with an attached eye tracker","Write using only the movement of the eyes! Try our dwell-free eye typing technique and see how fast you can type!","","118","Filteryedping: A Dwell-Free Eye Typing Technique","Diogo Pedrosa","Khai Truong","earlyrelease","","FormatComplete","2012/01510-0","FAPESP","CNPq","311659/2011-0","","","Feb 15 09:13",""
"int166","A","VoroGraph: Visualization Tools for Epidemic Analysis","Cody","Dunne","cdunne@us.ibm.com","int0166-paper.pdf","4","letter","","","Cody Dunne, Michael Muller, Nicola Perra, Mauro Martino","cdunne@us.ibm.com, michael_muller@us.ibm.com, n.perra@neu.edu, mmartino@us.ibm.com","17375","Cody","","Dunne","cdunne@us.ibm.com","","IBM Watson","Cambridge","Massachusetts","United States","","","","","","1047","Michael","","Muller","michael_muller@us.ibm.com","IBM Research, Cambridge, Massachusetts, United States","","","","","","","","","","45826","Nicola","","Perra","n.perra@neu.edu","Department of Health Sciences and College of Computer and Information Sciences","Northeastern University","Boston","Massachusetts","United States","","","","","","45827","Mauro","","Martino","mmartino@us.ibm.com","","IBM Watson","Cambridge","Massachusetts","United States","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Epidemiologists struggle to integrate complex information about the incidence and spread of disease, in relation to population density and other demographic conditions, at geographical scales ranging from global air travel down to local commuting. A partial solution overlays air travel as arcs above color-coded maps. However, commuting is not shown and it is often challenging to understand changing relationships due to the visual complexity arcs introduce. Moreover, when region sizes and shapes vary their color-codings become difficult to perceive. We introduce three visualizations which combine representations of population, movement, and disease spread at a local scale that is consistent with a zoomable global scale: (1) a map with commuting border encodings, (2) a centroidal Voronoi tessellation morphing technique, and (3) a meta-layout showing commuting alongside air travel. Our work provides mid-level abstractions that expert epidemiologists can use for insights into contagion.","Cody Dunne","cdunne@us.ibm.com","1. Balcan, D., Colizza, V., Gonçalves, B., Hu, H., Ramasco, J. J., and Vespignani, A. Multiscale mobility networks and the spatial spreading of infectious diseases. PNAS, 106(51), 2009: 21484–21489. \ 2. Du, Q., Faber, V., and Gunzburger, M. Centroidal Voronoi tessellations: applications and algorithms. SIAM Review, 41(4), 1999: 637–676. \ 3. Heer, J., and Robertson, G. G. Animated transitions in statistical data graphics. TVCG, 13(6), 2007: 1240–1247. \ 4. Lloyd, S. P. Least squares quantization in PCM. ToIT, 28(2), 1982: 129–137. \ ","Epidemiology; Emergency Management; Visualization; Graph/Network Data; Geographic/Geospatial Data","H.5.m","int0166-file1.zip","int0166-file2.jpg","int0166-file3.mp4","int0166-file4.png","","VoroGraph visualization technique showing borders between regions and epidemic propagation.","Epidemiologists study the spread of disease, whether by air travel or commuting. VoroGraph helps them follow the outbreak by morphing standard maps in several ways. Come try it for yourself!","","102","VoroGraph: Visualization Tools for Epidemic Analysis","Cody Dunne","Michael Muller","earlyrelease","","FormatComplete","","","","","","","Feb  2 22:50",""
"int167","A","삶 (“Salm”, “To Live”): Gaze Reactive Typography Inspired by Ahn Sang-Soo","Monchu","Chen","monchu@uma.pt","int0167-paper.pdf","4","letter","","","Monchu Chen, Bongkeum Jeong, Yoram I Chisik","monchu@uma.pt, bongkeum@gmail.com, ychisik@gmail.com","23285","Monchu","","Chen","monchu@uma.pt","Madeira-ITI","University of Madeira","Funchal","Madeira","Portugal","","","","","","46706","Bongkeum","","Jeong","bongkeum@gmail.com","Madeira-ITI","University of Madeira","Funchal","Madeira","Portugal","","","","","","8403","Yoram","I","Chisik","ychisik@gmail.com","","University of madeira","Funchal","","Portugal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","This research aims to explore the concept of gaze reactive typography, in which the design changes dynamically according to how audiences view it.  Inspired by the philosophy and styles of the famous Korean typographer Ahn Sang-Soo, we created an installation to showcase and exemplify relationships in four different levels between viewing behaviors and dynamic representations of typography.","Monchu Chen","monchu@uma.pt","1. Ahn, S. Typographic study of Yi Sang’s poetry. Hanyang University, (1996), 3-4. \ 2. Biedert, R., Buscher, G., Schwarz, S., Hees, J., Dengel, A,. Text 2.0, CHI '10 Extended Abstracts on Human Factors in Computing Systems, April 10-15, 2010, Atlanta, Georgia, USA \ 3. Ford, S., Forlizzi, J., Ishizaki, S., Kinetic typography: issues in time-based presentation of text, CHI '97 Extended Abstracts on Human Factors in Computing Systems, March 22-27, 1997, Atlanta, Georgia \ 4. Saiki, M. K., Ahn Sang-Soo: Going Home, Graphis Vol. 327. \ 5. Tywman, M. The British Library Guide to Printing. University of Toronto Press. 1999. Figure 6: A user trying the installation. \ ","Typography; Gaze-Based Interaction","H.5.m","int0167-file1.doc","int0167-file2.jpg","int0167-file3.mp4","int0167-file4.jpg","int0167-file5.mp4","Alphabets Forest typography design reacting to eye movements.","Come to experience how typography could dynamically react to different viewing behaviors.","","126","삶 (“Salm”, “To Live”): Gaze Reactive Typography Inspired by Ahn Sang-Soo","Monchu Chen","Bongkeum Jeong","earlyrelease","","FormatComplete","","","","","","","Feb 16 12:48",""
"int170","A","‘KIST Smart Wall’ and its Media Art Application: The Scenery Series","","","","int0170-paper.pdf","4","letter","","","Joong Ho Lee, Hyun Jhin Lee, Sanghwa Hong, Chungyo Ha, Ji-Hyung Park","yap153@gmail.com, hyunjhin@gmail.com, Doublebind8306@gmail.com, polyeffect@gmail.com, jhpark@kist.re.kr","27223","Joong Ho","","Lee","yap153@gmail.com","","Korea Institute of Science and Technology (KIST)","Seoul","Seoul","Republic of Korea","","","","","","50933","Hyun Jhin","","Lee","hyunjhin@gmail.com","","Hongik University","Sejong-si","","Korea, Republic of","","","","","","51157","Sanghwa","","Hong","Doublebind8306@gmail.com","","RECT2ELLIPSE Co.","Seoul","","Korea, Republic of","","","","","","51158","Chungyo","","Ha","polyeffect@gmail.com","","Seoul Electronic Design Group","Seoul","","Korea, Republic of","","","","","","16828","Ji-Hyung","","Park","jhpark@kist.re.kr","","Korea Institute of Science and Technology (KIST)","Seoul","Seoul","Republic of Korea","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","The interactive large displays for use in professional \ applications such as signage, education and installation \ art have been introduced for many years. However the \ innovative technology and novel application needed to \ drive the rapid growth of the market are still sought \ after. We suggest a series of media art application with \ the KIST Smart Wall which provides robust and highly \ functional multi-touch capability on the large display at \ low cost enabling to provide perspective scene for \ users. For this interactivity exhibition, we collaborated \ with three media artists to explore the interactive wall \ as an emotional and aesthetic media art platform. ‘The \ Scenery Series’ will communicate with audiences in \ emotionally touching images and meanings.","Ji-Hyung PARK","jhpark@kist.re.kr","1. J. H. Lee, Won Moon, Dongwook Yun, Ji-Hyung Park, Introduction of Infrared Array Scanning Method for Multi-Touch Integration into Large Scale Display Technologies. In Proc. ICCCT 2011 (2011). \ 2. New Oxford American Dictionary, Oxford University Press (2010). \ 3. Yoshio Nakamura, Creating Landscapes, Tokyo: NHK publications (2004) 31-35. \ 4. Paradiso, J. A., K. Hsiao, et al., Sensor systems for interactive surfaces. Ibm Systems Journal (2000) 39(34): 892-914. \ 5. Peltonen, P., E. Kurvinen, et al., ""It's Mine, Don't Touch!"": Interactions at a Large Multi-Touch Display in a City Centre. In Proc. CHI 2008 (2008). \ 6. Novak, J. and S. Schmidt, When Joy Matters: The Importance of Hedonic Stimulation in Collocated Collaboration with Large-Displays. Human-Computer Interaction - Interact 2009. 5727: 618-629. (2009). \ 7. Young Jo Kang, Step on Landscapes, Hyohyung press (2003) 76-89. \ 8. Jinkyung Sansoohwa, Encyclopedia of Korean Culture,http://encykorea.aks.ac.kr/Contents/Index?con tents_id=E0054575. (2015) Figure 5. The second theme of The Scenery Series: ‘2015 Jinkyung Sansoohwa’ Figure 6. The third theme of The Scenery Series: ‘Known Sea’ \ ","Multi-touch, Interactive Public Display, Interactive Media Art","H.5.m","int0170-file1.doc","","int0170-file3.mp4","int0170-file4.jpg","int0170-file5.mp4","","KIST Smart Wall provides robust and highly \ functional multi-touch capability on the large display showing ‘The \ Scenery Series’ to communicate with audiences in \ emotionally touching images and meanings.","","113","The Scenery Series: ‘Known Sea’","Ji-Hyung Park","Joong Ho Lee","earlyrelease","ContentComplete","","","","","","","","Mar 11 11:40",""
"int171","A","Comfort Zone","","","","int0171-paper.pdf","4","letter","","incomplete","Todd Holoubek","todd.holo@gmail.com","48046","Todd","","Holoubek","todd.holo@gmail.com","Visual Multimedia design","Sookmyung Womens University","Seoul","Yongsan","S Korea","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","This experiment explores human to human interaction through presenting a base example of an individual modeled with software to actual human individuals. It compares the real humans behaviors with the model and humans with humans.","Todd Holoubek","todd.holo@gmail.com","ERROR: Sorry, but we could not find the bibliography in your PDF file.  Please entry it manually. \ ","Interactive, art, zones of comfort","H.5.m","int0171-file1.docx","","","","","","The kinetic energy between two people is a mysterious space that exists in a constant state of flux. Each individual’s comfort level depends on the environment and other individuals nearby.  \ ","","115","","Todd Holoubek","Todd Holoubek","earlyrelease","ContentComplete","","","","","","","","Mar 11 10:17",""
"int172","A","RGB Color Bits","","","","int0172-paper.pdf","3","letter","","","Sanghwa Hong","doublebind8306@gmail.com","51157","Sanghwa","","Hong","doublebind8306@gmail.com","","RECT2ELLIPSE Co.","Seoul","","Korea, Republic of","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","RGB Color bits is an animated RGB LED panel which \ represents a mutual understanding of recognition between \ analog colors and 8 bit color data of machine. We, humans \ are surrounded by digital displays in this era and high \ resolution of the digital display technology is blurring the \ lines between real and virtual world. \ Images that we’ve seen though the digital displays look \ almost real but they are obviously composed of different \ elements, being consistent of code and data. For example, \ the true red color on monitor display and colored paper \ could have the same appearance but the inside composing \ elements are totally different. RGB Color Bit tried to show \ the constituents of digital color by physical RGB LEDs and \ 8 bits dots punched out of wooden panel.","Sanghwa Hong","Doublebind8306@gmail.com","1. Touchable Colors. http://www.sang2h.com/index.php?/mediaart/tochable colors/ \ 2. Wooden Touchable Colors http://www.sang2h.com/index.php?/mediaart/woodentochable-colors/ \ 3. Classical Element http://en.wikipedia.org/wiki/Classical_element \ 4. Superstring Theory http://en.wikipedia.org/wiki/Superstring_theory \ ","Media Art, Virtual, Elements","H.5.m","int0172-file1.doc","","int0172-file3.mp4","int0172-file4.jpg","int0172-file5.mp4","","RGB Color bits is an animated RGB LED panel which \ represents a mutual understanding of recognition between \ analog colors and 8 bit color data of machine.","","131","RGB Color Bits","Sanghwa Hong","Sanghwa Hong","earlyrelease","ContentComplete","","","","","","","","Mar 11 05:09",""
"int173","A","Visual Liquidizer or Virtual Merge","","","","int0173-paper.pdf","4","letter","","","Tatsuo Unemi, Daniel Bisig","unemi@soka.ac.jp, daniel.bisig@zhdk.ch","51156","Tatsuo","","Unemi","unemi@soka.ac.jp","Department of Information Systems Science","Soka University","Hachioji","Tokyo","Japan","","","","","","51160","Daniel","","Bisig","daniel.bisig@zhdk.ch","Institute for Computer Music and Sound Technology","Zurich University of the Arts","Zurich","","Switzerland","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","The authors’ latest artwork entitled Visual Liquidizer or \ Virtual Merge is a new form of audio-visual interactive \ installation that displays a deformed dynamic images of \ visitors as if their bodies become liquidized, scattered, and \ mixed. It is intended to provide a virtual experience of \ deeper contact with the other persons. The basic idea was \ inspired by a science fiction Wetware by R. Rucker. The \ authors developed an algorithm using two types of swarm \ simulations, ANT and BOIDS, in order to realize \ deformation of living fragments. Sound effects are \ generated synchronously with visuals by mixing sampled \ sounds of water flows with the visitors’ voice. It achieved \ quick response for realtime interaction utilizing parallel \ processing with CPU and GPU.","Tatsuo Unemi"," unemi@soka.ac.jp","1. Bisig, D., and Unemi, T. Mediaﬂies – a video and audio remixing multi agent system. In Proc. of the 9th Generative Art Conference, C. Soddu, Ed. (Milan, Italy, 2006). \ 2. Bisig, D., and Unemi, T. Swarms on stage – swarm simulations for dance performance. In Proc. of the 12th Generative Art Conference, C. Soddu, Ed. (Milan, Italy, 2009). \ 3. Bisig, D., and Unemi, T. Cycles – blending natural and artiﬁcial properties in a generative artwork. In Proc. of the 13th Generative Art Conference, C. Soddu, Ed. (Milan, Italy, 2010). \ 4. Blackwell, T. M., and Bentley, P. Improvised music with swarms. In Proc. of the Congress on Evolutionary Computation (2002), 1462–1467. \ 5. Dorigo, M., and St¨utzle, T. Ant Colony Optimization. MIT Press/Bradford Books, Cambridge, MA, 2004. \ 6. Jacob, C., Hushlak, G., Boyd, J., Nuytten, P., Sayles, M., and Pilat, M. SwarmArt: Interactive art from swarm intelligence. Leonardo 40, 3 (2007), 248–255. \ 7. Resnick, M. Turtles, Termites, and Traﬃc Jams: Explorations in Massively Parallel Microworlds. MIT Press, Cambridge, MA, 1994. \ 8. Reynolds, C. W. Flocks, herds, and schools: a distributed behavioral model. SIGGRAPH Computer Graphics 21, 4 (1987), 25–34. \ 9. Rucker, R. Wetware. Avon Books, New York City, 1988. republished as a part of The ware tetralogy, Prime Books, 2010. \ 10. Shiﬀman, D. Swarm, 2002. http://shiffman.net/projects/swarm/. \ 11. Unemi, T., and Bisig, D. Visual deformation by swarm – a technique for virtual liquidizer of objects. In Proc. of the 17th Generative Art Conference, C. Soddu, Ed. (Rome, Italy, 2014), 347–356. \ 12. Unemi, T., and Bisig, D. Visual liquidizer or virtual merge #1. In Proc. of the 17th Generative Art Conference, C. Soddu, Ed. (Rome, Italy, 2014), 376–381. \ 13. Unemi, T., Matsui, Y., and Bisig, D. Identity SA 1.6 – an artistic software that produces a deformed audiovisual reﬂection based on a visually interactive swarm. In Proc. of the ACE 2008 International Conference on Advances in Computer Entertainment Technology (Yokohama, Japan, 2008), 297–300. \ ","Swarm simulation, visual interaction, deformation","H.5.m","int0173-file1.zip","","int0173-file3.mp4","int0173-file4.jpg","int0173-file5.mp4","","Visual Liquidizer displays deformed dynamic images of \ visitors as if their bodies become liquidized, scattered, and \ mixed.","","129","Visual Liquidizer or Virtual Merge","Tatsuo Unemi","Daniel Bisig","earlyrelease","ContentComplete","","","","","","","","Mar 11 05:06",""
"int175","A","mood.cloud: Data as Art","","","","int0175-paper.pdf","4","letter","","","Younghui Kim, Geri Gay, Lindsay Reynolds, Hyuns Hong","ihongik@gmail.com, gkg1@cornell.edu, llr48@cornell.edu, i@lovot.net","33730","Younghui","","Kim","ihongik@gmail.com","WCU Digital Media Public Art Research Lab","Hongik University","Sejong","Chungnam","South Korea","","","","","","2332","Geri","","Gay","gkg1@cornell.edu","","Cornell University","Ithaca","New York","United States","","","","","","15382","Lindsay","","Reynolds","llr48@cornell.edu","","Cornell University","Ithaca","New York","United States","","","","","","51159","Hyuns ","","Hong","i@lovot.net","","Hongik University","Sejong","","Korea, Republic of","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","The project “mood.cloud” is a LED light sculpture \ created through a collaborative effort among artists and \ information scientists. This light installation reflects the \ mood of people in a public space. For this exhibit, \ people selected an image on an iPad that best \ represented their current mood. The colors of the \ sculpture changed reflecting not only the individual's \ mood but the mood of others in the space. \ The “mood.cloud” platform can be re-programmed to \ not only capture represent emotional status in different \ ways, but also has the potential to track many different \ kinds of input.","Younghui Kim"," ihongik@gmail.com","1. J.P. Pollak, Phil Adams, Geri Gay, PAM: A Photographic Affect Meter for Frequent, In Situ Measurement of Affect, CHI 2011, ACM Press (2011) \ 2. Data as Art, http://cis.cornell.edu/data-art \ ","Data Art; Light Installation; Interactive Art;","H.5.m","int0175-file1.doc","","int0175-file3.mp4","int0175-file4.jpg","int0175-file5.mp4","","“mood.cloud” is a LED light sculpture \ that reflects the \ mood of people in a public space.","","130","mood.cloud","Younghui Kim","Hyuns  Hong","earlyrelease","ContentComplete","","","","","","","","Mar 11 05:11",""
